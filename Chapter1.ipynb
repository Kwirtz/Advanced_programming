{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Webscrapping and api.\n",
    "\n",
    "In this chapter we will go more in depth on the scraping methodology. First we will go back to Beautiful Soup with a more complex example, we then discuss the advantages of selenium and scrappy and move on to the API part of the course. \n",
    "\n",
    "Structure:\n",
    "- [Web developping tools in the webbrowser](#WB)\n",
    "- [Beautiful Soup](#BS)\n",
    "- [Scrapy](#Scrapy)\n",
    "- [Selenium](#Selenium)\n",
    "- [APIs](#APIs)\n",
    "- [Rules of good conduct](#rules)\n",
    "- [TODO](#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"WB\"></a>\n",
    "## Web developping tools in the webbrowser\n",
    "\n",
    "We have seen in introduction how to download html and with Beautiful Soup (BS) and read it with prettify. In practice it can be hard to find what you want using this method. An other option is to use web developping tools available on every browser (Google chrome, Mozilla, ...). Here's a quick introduction to what you can do on browser and how it can help you (note: i'll be using Mozilla)\n",
    "\n",
    "- Ctrl+u   -> Watching the source code generating the page (What we get with requests.get() )\n",
    "- Ctrl+Maj+C -> Inspector, Hover on element to see where it is on html page\n",
    "- Maj+F7 -> Style editor, Check the css the page is using\n",
    "- Ctrl+Maj+E -> Network, see what you are loading when opening a page (important for JS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"BS\"></a>\n",
    "## Beautiful Soup\n",
    "\n",
    "So we have seen the basic usage of BS as a reminder of last year, let's move on a bigger project. You are probably familiar with the 6 degrees of separation ? Number of \"steps\" (friends of friends) between two individuals is 6 or fewer. Our goal will be to scrap a wikipedia page, get all the href and continue this process until 6 layers deep. (This idea comes from Mitchell R. Web scraping with Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "import re # regex expression\n",
    "import tqdm # time loop in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/wiki/Kevin_Bacon_(disambiguation)\n",
      "/wiki/Philadelphia\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "/wiki/Holly_Near\n",
      "/wiki/Leading_man\n",
      "/wiki/Character_actor\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/Screen_Actors_Guild_Award\n",
      "/wiki/Primetime_Emmy_Award\n",
      "/wiki/National_Lampoon%27s_Animal_House\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/Diner_(1982_film)\n",
      "/wiki/JFK_(film)\n",
      "/wiki/A_Few_Good_Men\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Frost/Nixon_(film)\n",
      "/wiki/Friday_the_13th_(1980_film)\n",
      "/wiki/Tremors_(1990_film)\n",
      "/wiki/The_River_Wild\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Crazy,_Stupid,_Love\n",
      "/wiki/Patriots_Day_(film)\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Michael_Strobl\n",
      "/wiki/HBO\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Fox_Broadcasting_Company\n",
      "/wiki/The_Following\n",
      "/wiki/Amazon_Prime_Video\n",
      "/wiki/I_Love_Dick_(TV_series)\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Television_Series_Musical_or_Comedy\n",
      "/wiki/Showtime_(TV_network)\n",
      "/wiki/City_on_a_Hill_(TV_series)\n",
      "/wiki/The_Guardian\n",
      "/wiki/Academy_Award\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/EE_Limited\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Philadelphia\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Urban_planning\n",
      "/wiki/Design_of_Cities\n",
      "/wiki/Julia_R._Masterman_School\n",
      "/wiki/Spring_Garden,_Philadelphia\n",
      "/wiki/Pennsylvania_Governor%27s_School_for_the_Arts\n",
      "/wiki/Bucknell_University\n",
      "/wiki/Lewisburg,_Pennsylvania\n",
      "/wiki/Glory_Van_Scott\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Circle_in_the_Square\n",
      "/wiki/Nancy_Mills\n",
      "/wiki/Cosmopolitan_(magazine)\n",
      "/wiki/Fraternities_and_sororities\n",
      "/wiki/Animal_House\n",
      "/wiki/Search_for_Tomorrow\n",
      "/wiki/Guiding_Light\n",
      "/wiki/Friday_the_13th_(1980_film)\n",
      "/wiki/Getting_Out\n",
      "/wiki/Phoenix_Theater\n",
      "/wiki/Flux\n",
      "/wiki/Second_Stage_Theatre\n",
      "/wiki/Obie_Award\n",
      "/wiki/Forty_Deuce\n",
      "/wiki/Slab_Boys\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Val_Kilmer\n",
      "/wiki/Barry_Levinson\n",
      "/wiki/Diner_(1982_film)\n",
      "/wiki/Steve_Guttenberg\n",
      "/wiki/Daniel_Stern_(actor)\n",
      "/wiki/Mickey_Rourke\n",
      "/wiki/Tim_Daly\n",
      "/wiki/Ellen_Barkin\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/James_Dean\n",
      "/wiki/Rebel_Without_a_Cause\n",
      "/wiki/Mickey_Rooney\n",
      "/wiki/Judy_Garland\n",
      "/wiki/Typecasting_(acting)\n",
      "/wiki/John_Hughes_(filmmaker)\n",
      "/wiki/Planes,_Trains_and_Automobiles\n",
      "/wiki/She%27s_Having_a_Baby\n",
      "/wiki/The_Big_Picture_(1989_film)\n",
      "/wiki/Tremors_(1990_film)\n",
      "/wiki/Joel_Schumacher\n",
      "/wiki/Flatliners\n",
      "/wiki/Elizabeth_Perkins\n",
      "/wiki/He_Said,_She_Said_(film)\n",
      "/wiki/The_New_York_Times\n",
      "/wiki/Oliver_Stone\n",
      "/wiki/JFK_(film)\n",
      "/wiki/A_Few_Good_Men_(film)\n",
      "/wiki/Michael_Greif\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/The_River_Wild\n",
      "/wiki/Meryl_Streep\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/Blockbuster_(entertainment)\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Sleepers_(film)\n",
      "/wiki/Picture_Perfect_(1997_film)\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Digging_to_China\n",
      "/wiki/Payola\n",
      "/wiki/Telling_Lies_in_America_(film)\n",
      "/wiki/Wild_Things_(film)\n",
      "/wiki/Stir_of_Echoes\n",
      "/wiki/David_Koepp\n",
      "/wiki/Cannes_Film_Festival\n",
      "/wiki/Paul_Verhoeven\n",
      "/wiki/Hollow_Man\n",
      "/wiki/Colin_Firth\n",
      "/wiki/Rachel_Blanchard\n",
      "/wiki/M%C3%A9nage_%C3%A0_trois\n",
      "/wiki/Where_the_Truth_Lies\n",
      "/wiki/Atom_Egoyan\n",
      "/wiki/MPAA\n",
      "/wiki/MPAA_film_rating_system\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Tim_Robbins\n",
      "/wiki/Clint_Eastwood\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/HBO_Films\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Michael_Strobl\n",
      "/wiki/Desert_Storm\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Toronto_Film_Festival\n",
      "/wiki/Matthew_Vaughn\n",
      "/wiki/Sebastian_Shaw_(comics)\n",
      "/wiki/Dustin_Lance_Black\n",
      "/wiki/8_(play)\n",
      "/wiki/Perry_v._Brown\n",
      "/wiki/Proposition_8\n",
      "/wiki/Charles_J._Cooper\n",
      "/wiki/Wilshire_Ebell_Theatre\n",
      "/wiki/American_Foundation_for_Equal_Rights\n",
      "/wiki/The_Following\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Huffington_Post\n",
      "/wiki/Tremors_(1990_film)\n",
      "/wiki/The_Bacon_Brothers\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "/wiki/Instagram\n",
      "/wiki/Old_97%27s\n",
      "/wiki/The_Guardians_of_the_Galaxy_Holiday_Special\n",
      "/wiki/EE_(telecommunications_company)\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/Trivia\n",
      "/wiki/Big_screen\n",
      "/wiki/Six_degrees_of_separation\n",
      "/wiki/Internet_meme\n",
      "/wiki/SixDegrees.org\n",
      "/wiki/Social_networking_service\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/IMDb\n",
      "/wiki/Paul_Erd%C5%91s\n",
      "/wiki/Erd%C5%91s_number\n",
      "/wiki/Paul_Erd%C5%91s\n",
      "/wiki/Erd%C5%91s_number\n",
      "/wiki/Erd%C5%91s%E2%80%93Bacon_number\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/PBS\n",
      "/wiki/Lanford_Wilson\n",
      "/wiki/Lemon_Sky\n",
      "/wiki/Pyrates\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Upper_West_Side\n",
      "/wiki/Manhattan\n",
      "/wiki/Tracy_Pollan\n",
      "/wiki/Separation_of_church_and_state_in_the_United_States\n",
      "/wiki/The_Times\n",
      "/wiki/Atheism\n",
      "/wiki/Antireligion\n",
      "/wiki/Will.i.am\n",
      "/wiki/It%27s_a_New_Day_(Will.i.am_song)\n",
      "/wiki/Barack_Obama\n",
      "/wiki/Ponzi_scheme\n",
      "/wiki/Bernie_Madoff\n",
      "/wiki/Finding_Your_Roots\n",
      "/wiki/Henry_Louis_Gates\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Blockbuster_Entertainment_Awards\n",
      "/wiki/Blockbuster_Entertainment_Awards\n",
      "/wiki/Hollow_Man\n",
      "/wiki/Boston_Society_of_Film_Critics\n",
      "/wiki/Boston_Society_of_Film_Critics_Award_for_Best_Cast\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Bravo_Otto\n",
      "/wiki/Bravo_Otto\n",
      "/wiki/Footloose_(1984_film)\n",
      "/wiki/CableACE_Award\n",
      "/wiki/CableACE_Award\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Chlotrudis_Awards\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Critics%27_Choice_Movie_Awards\n",
      "/wiki/Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/Ghent_International_Film_Festival\n",
      "/wiki/Ghent_International_Film_Festival\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Giffoni_Film_Festival\n",
      "/wiki/Giffoni_Film_Festival\n",
      "/wiki/Digging_to_China\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Golden_Globe_Award\n",
      "/wiki/Golden_Globe_Award_for_Best_Supporting_Actor_%E2%80%93_Motion_Picture\n",
      "/wiki/The_River_Wild\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Television_Series_Musical_or_Comedy\n",
      "/wiki/I_Love_Dick_(TV_series)\n",
      "/wiki/Independent_Spirit_Awards\n",
      "/wiki/Independent_Spirit_Award_for_Best_Male_Lead\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/MTV_Movie_%26_TV_Awards\n",
      "/wiki/MTV_Movie_Award_for_Best_Villain\n",
      "/wiki/Hollow_Man\n",
      "/wiki/Taking_Chance\n",
      "/wiki/The_Following\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/The_Following\n",
      "/wiki/E!_People%27s_Choice_Awards\n",
      "/wiki/The_Following\n",
      "/wiki/Primetime_Emmy_Award\n",
      "/wiki/Primetime_Emmy_Award_for_Outstanding_Lead_Actor_in_a_Limited_Series_or_Movie\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Satellite_Awards\n",
      "/wiki/Satellite_Award_for_Best_Actor_%E2%80%93_Motion_Picture\n",
      "/wiki/The_Woodsman_(2004_film)\n",
      "/wiki/Satellite_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Saturn_Award\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/The_Following\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/The_Following\n",
      "/wiki/Scream_Awards\n",
      "/wiki/Scream_Awards\n",
      "/wiki/Screen_Actors_Guild_Award\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Supporting_Role\n",
      "/wiki/Murder_in_the_First_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Apollo_13_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Mystic_River_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Cast_in_a_Motion_Picture\n",
      "/wiki/Frost/Nixon_(film)\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Taking_Chance\n",
      "/wiki/Teen_Choice_Awards\n",
      "/wiki/Teen_Choice_Award_for_Choice_Movie_Villain\n",
      "/wiki/Beauty_Shop\n",
      "/wiki/Teen_Choice_Award_for_Choice_Movie_Villain\n",
      "/wiki/TV_Guide_Award\n",
      "/wiki/TV_Guide_Award\n",
      "/wiki/The_Following\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "/wiki/Hollywood_Walk_of_Fame\n",
      "/wiki/Denver_Film_Festival\n",
      "/wiki/Phoenix_Film_Festival\n",
      "/wiki/Santa_Barbara_International_Film_Festival\n",
      "/wiki/Broadcast_Film_Critics_Association\n",
      "/wiki/Seattle_International_Film_Festival\n",
      "/wiki/List_of_actors_with_Hollywood_Walk_of_Fame_motion_picture_stars\n",
      "/wiki/The_Hollywood_Reporter\n",
      "/wiki/The_Austin_Chronicle\n",
      "/wiki/Access_Hollywood\n",
      "/wiki/CNN\n",
      "/wiki/IMDb_(identifier)\n",
      "/wiki/Internet_Broadway_Database\n",
      "/wiki/Internet_Off-Broadway_Database\n",
      "/wiki/AllMovie\n",
      "/wiki/Kevin_Bacon_filmography\n",
      "/wiki/Losing_Chase\n",
      "/wiki/Loverboy_(2005_film)\n",
      "/wiki/Kyra_Sedgwick\n",
      "/wiki/Sosie_Bacon\n",
      "/wiki/Edmund_Bacon_(architect)\n",
      "/wiki/Michael_Bacon_(musician)\n",
      "/wiki/The_Bacon_Brothers\n",
      "/wiki/Six_Degrees_of_Kevin_Bacon\n",
      "/wiki/Erd%C5%91s%E2%80%93Bacon_number\n",
      "/wiki/SixDegrees.org\n",
      "/wiki/Critics%27_Choice_Movie_Award_for_Best_Actor\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Jack_Nicholson\n",
      "/wiki/Ian_McKellen\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Jack_Nicholson\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Jamie_Foxx\n",
      "/wiki/Philip_Seymour_Hoffman\n",
      "/wiki/Forest_Whitaker\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Sean_Penn\n",
      "/wiki/Jeff_Bridges\n",
      "/wiki/Colin_Firth\n",
      "/wiki/George_Clooney\n",
      "/wiki/Daniel_Day-Lewis\n",
      "/wiki/Matthew_McConaughey\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Leonardo_DiCaprio\n",
      "/wiki/Casey_Affleck\n",
      "/wiki/Gary_Oldman\n",
      "/wiki/Christian_Bale\n",
      "/wiki/Joaquin_Phoenix\n",
      "/wiki/Chadwick_Boseman\n",
      "/wiki/Will_Smith\n",
      "/wiki/Brendan_Fraser\n",
      "/wiki/Golden_Globe_Award_for_Best_Actor_%E2%80%93_Miniseries_or_Television_Film\n",
      "/wiki/Mickey_Rooney\n",
      "/wiki/Anthony_Andrews\n",
      "/wiki/Richard_Chamberlain\n",
      "/wiki/Ted_Danson\n",
      "/wiki/Dustin_Hoffman\n",
      "/wiki/James_Woods\n",
      "/wiki/Randy_Quaid\n",
      "/wiki/Michael_Caine\n",
      "/wiki/Stacy_Keach\n",
      "/wiki/Robert_Duvall\n",
      "/wiki/James_Garner\n",
      "/wiki/Beau_Bridges\n",
      "/wiki/Robert_Duvall\n",
      "/wiki/James_Garner\n",
      "/wiki/Raul_Julia\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Alan_Rickman\n",
      "/wiki/Ving_Rhames\n",
      "/wiki/Stanley_Tucci\n",
      "/wiki/Jack_Lemmon\n",
      "/wiki/Brian_Dennehy\n",
      "/wiki/James_Franco\n",
      "/wiki/Albert_Finney\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Jonathan_Rhys_Meyers\n",
      "/wiki/Bill_Nighy\n",
      "/wiki/Jim_Broadbent\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Idris_Elba\n",
      "/wiki/Kevin_Costner\n",
      "/wiki/Michael_Douglas\n",
      "/wiki/Billy_Bob_Thornton\n",
      "/wiki/Oscar_Isaac\n",
      "/wiki/Tom_Hiddleston\n",
      "/wiki/Ewan_McGregor\n",
      "/wiki/Darren_Criss\n",
      "/wiki/Russell_Crowe\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Evan_Peters\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Saturn_Award_for_Best_Actor_on_Television\n",
      "/wiki/Kyle_Chandler\n",
      "/wiki/Steven_Weber_(actor)\n",
      "/wiki/Richard_Dean_Anderson\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/Robert_Patrick\n",
      "/wiki/Ben_Browder\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/David_Boreanaz\n",
      "/wiki/Ben_Browder\n",
      "/wiki/Matthew_Fox\n",
      "/wiki/Michael_C._Hall\n",
      "/wiki/Matthew_Fox\n",
      "/wiki/Edward_James_Olmos\n",
      "/wiki/Josh_Holloway\n",
      "/wiki/Stephen_Moyer\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Mads_Mikkelsen\n",
      "/wiki/Hugh_Dancy\n",
      "/wiki/Andrew_Lincoln\n",
      "/wiki/Bruce_Campbell\n",
      "/wiki/Andrew_Lincoln\n",
      "/wiki/Kyle_MacLachlan\n",
      "/wiki/Patrick_Stewart\n",
      "/wiki/Saturn_Award_for_Best_Actor_in_a_Network_or_Cable_Television_Series\n",
      "/wiki/Sam_Heughan\n",
      "/wiki/Bob_Odenkirk\n",
      "/wiki/Saturn_Award_for_Best_Actor_in_a_Streaming_Television_Series\n",
      "/wiki/Henry_Thomas\n",
      "/wiki/Oscar_Isaac\n",
      "/wiki/Screen_Actors_Guild_Award_for_Outstanding_Performance_by_a_Male_Actor_in_a_Miniseries_or_Television_Movie\n",
      "/wiki/Raul_Julia\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Alan_Rickman\n",
      "/wiki/Gary_Sinise\n",
      "/wiki/Christopher_Reeve\n",
      "/wiki/Jack_Lemmon\n",
      "/wiki/Brian_Dennehy\n",
      "/wiki/Ben_Kingsley\n",
      "/wiki/William_H._Macy\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Geoffrey_Rush\n",
      "/wiki/Paul_Newman\n",
      "/wiki/Jeremy_Irons\n",
      "/wiki/Kevin_Kline\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Al_Pacino\n",
      "/wiki/Paul_Giamatti\n",
      "/wiki/Kevin_Costner\n",
      "/wiki/Michael_Douglas\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Idris_Elba\n",
      "/wiki/Bryan_Cranston\n",
      "/wiki/Alexander_Skarsg%C3%A5rd\n",
      "/wiki/Darren_Criss\n",
      "/wiki/Sam_Rockwell\n",
      "/wiki/Mark_Ruffalo\n",
      "/wiki/Michael_Keaton\n",
      "/wiki/Sam_Elliott\n"
     ]
    }
   ],
   "source": [
    "# Starting from the wikipedia page of Kevin Bacon\n",
    "starting_url = \"https://en.wikipedia.org/wiki/Kevin_Bacon\"\n",
    "\n",
    "# Get html content\n",
    "response = requests.get(starting_url)\n",
    "result = response.content\n",
    "\n",
    "# Parse html with BS\n",
    "soup = BeautifulSoup(result, 'html.parser')\n",
    "\n",
    "# In the body content find all href that matches the regex query (start with wiki and ignore !: to avoid artifacts like jpeg )\n",
    "for link in soup.find(\"div\",attrs={'id':'bodyContent'}).find_all(\"a\",href = re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "    print(link.get(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using function so that it is cleaner\n",
    "\n",
    "\n",
    "def Get_hrefs(url):\n",
    "    # Request url and create bs object.\n",
    "    response = requests.get(url)\n",
    "    result = response.content    \n",
    "    soup = BeautifulSoup(result, 'html.parser')\n",
    "    \n",
    "    # init the list with all href\n",
    "    hrefs = []\n",
    "    for link in soup.find(\"div\",attrs={'id':'bodyContent'}).find_all(\"a\",href = re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "        if \"href\" in link.attrs:\n",
    "            if link.get(\"href\") not in hrefs:\n",
    "                hrefs.append(link.get(\"href\"))\n",
    "    return(hrefs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [01:26<00:00,  3.37it/s]\n",
      "100%|██████████| 2/2 [01:26<00:00, 43.47s/it]\n"
     ]
    }
   ],
   "source": [
    "# depth = number of times we get the hrefs of the hrefs.\n",
    "# We limit at 2 to not overlead wikipedia with our things but in theory depth of 6 and you could have every person ?\n",
    "depth = 2\n",
    "\n",
    "# hrefs_checked = keeping track of href already visited\n",
    "hrefs_checked = []\n",
    "\n",
    "for i in tqdm.tqdm(range(depth)):\n",
    "    # First iteration start from Kevin Bacon\n",
    "    if i == 0:\n",
    "        starting_url = \"https://en.wikipedia.org/wiki/Kevin_Bacon\"\n",
    "        hrefs = Get_hrefs(starting_url)\n",
    "        hrefs_checked.append(starting_url)\n",
    "    else:\n",
    "        hrefs_temp = []\n",
    "        for starting_url in tqdm.tqdm(hrefs):\n",
    "            url = \"https://en.wikipedia.org\" + starting_url\n",
    "            # Checking if url not visited. Could become inneficient\n",
    "            if url not in hrefs_checked:\n",
    "                hrefs_temp += Get_hrefs(url)\n",
    "        hrefs += [href for href in hrefs_temp if href not in hrefs_checked]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "118943"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hrefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A step further we want to process text and save it in MongoDB\n",
    "# Also short intro into classes\n",
    "import pymongo \n",
    "\n",
    "class crawler:\n",
    "    def __init__(self,starting_url, depth, mongo_uri, db_name, collection_name ):\n",
    "        self.starting_url = starting_url\n",
    "        self.depth = depth\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.db_name = db_name\n",
    "        self.collection_name = collection_name\n",
    "        self.hrefs_checked = []\n",
    "        self.n_processed = 0\n",
    "        \n",
    "    def Get_hrefs(self,url):\n",
    "\n",
    "        hrefs = []\n",
    "        for link in self.soup.find(\"div\",attrs={'id':'bodyContent'}).find_all(\"a\",href = re.compile(\"^(/wiki/)((?!:).)*$\")):\n",
    "            if \"href\" in link.attrs:\n",
    "                if link.get(\"href\") not in hrefs:\n",
    "                    hrefs.append(link.get(\"href\"))\n",
    "        return(hrefs)\n",
    "    \n",
    "    def parse_url(self): \n",
    "        full_text = \"\"\n",
    "        for para in self.soup.find_all(\"p\"):\n",
    "            full_text += para.text + \" \"\n",
    "        return(full_text)\n",
    "    \n",
    "    def save2mongo(self):\n",
    "        Client = pymongo.MongoClient(self.mongo_uri)\n",
    "        db = Client[self.db_name]\n",
    "        collection = db[self.collection_name]\n",
    "        \n",
    "        collection.insert_many(self.list_of_insertion)\n",
    "        \n",
    "    def run_analysis(self):\n",
    "        \n",
    "        self.list_of_insertion = []\n",
    "        \n",
    "        for i in tqdm.tqdm(range(self.depth)):\n",
    "            # First iteration start from Kevin Bacon\n",
    "            if i == 0:\n",
    "                response = requests.get(self.starting_url)\n",
    "                result = response.content    \n",
    "                self.soup = BeautifulSoup(result, 'html.parser')\n",
    "                hrefs = self.Get_hrefs(self.starting_url)\n",
    "                text = self.parse_url()\n",
    "                self.hrefs_checked.append(self.starting_url)\n",
    "                self.n_processed += 1\n",
    "                self.list_of_insertion.append({\"id\":self.n_processed, \"text\" : text, \"href\":self.starting_url})\n",
    "            else:\n",
    "                hrefs_temp = []\n",
    "                for starting_url in tqdm.tqdm(hrefs):\n",
    "                    url = \"https://en.wikipedia.org\" + starting_url\n",
    "                    response = requests.get(url)\n",
    "                    result = response.content    \n",
    "                    self.soup = BeautifulSoup(result, 'html.parser')\n",
    "                    hrefs_temp += self.Get_hrefs(url)\n",
    "                    text = self.parse_url()\n",
    "                    self.n_processed += 1\n",
    "                    self.list_of_insertion.append({\"id\":self.n_processed, \"text\" : text, \"href\":url})\n",
    "                    if len(self.list_of_insertion) % 200 == 0:\n",
    "                        self.save2mongo()\n",
    "                        self.list_of_insertion = []\n",
    "                hrefs = [href for href in hrefs_temp if href not in self.hrefs_checked]      \n",
    "        self.save2mongo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 292/292 [01:08<00:00,  4.29it/s]\n",
      " 11%|█         | 14523/131974 [1:43:46<13:59:18,  2.33it/s]\n",
      " 67%|██████▋   | 2/3 [1:44:55<52:27, 3147.61s/it]\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\requests\\adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[0;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[0;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[0;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[0;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    842\u001b[0m     new_e \u001b[39m=\u001b[39m ProtocolError(\u001b[39m\"\u001b[39m\u001b[39mConnection aborted.\u001b[39m\u001b[39m\"\u001b[39m, new_e)\n\u001b[1;32m--> 844\u001b[0m retries \u001b[39m=\u001b[39m retries\u001b[39m.\u001b[39;49mincrement(\n\u001b[0;32m    845\u001b[0m     method, url, error\u001b[39m=\u001b[39;49mnew_e, _pool\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m, _stacktrace\u001b[39m=\u001b[39;49msys\u001b[39m.\u001b[39;49mexc_info()[\u001b[39m2\u001b[39;49m]\n\u001b[0;32m    846\u001b[0m )\n\u001b[0;32m    847\u001b[0m retries\u001b[39m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\util\\retry.py:470\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[39mif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m \u001b[39mor\u001b[39;00m method \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 470\u001b[0m     \u001b[39mraise\u001b[39;00m reraise(\u001b[39mtype\u001b[39;49m(error), error, _stacktrace)\n\u001b[0;32m    471\u001b[0m \u001b[39melif\u001b[39;00m read \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39m__traceback__ \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[39mraise\u001b[39;00m value\u001b[39m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[39mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[0;32m    791\u001b[0m     conn,\n\u001b[0;32m    792\u001b[0m     method,\n\u001b[0;32m    793\u001b[0m     url,\n\u001b[0;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[0;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[0;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[0;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[0;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[0;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[0;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[0;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[0;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[0;32m    803\u001b[0m )\n\u001b[0;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[0;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\http\\client.py:1374\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1374\u001b[0m     response\u001b[39m.\u001b[39;49mbegin()\n\u001b[0;32m   1375\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[0;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\http\\client.py:287\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m line:\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m     \u001b[39mraise\u001b[39;00m RemoteDisconnected(\u001b[39m\"\u001b[39m\u001b[39mRemote end closed connection without\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m                              \u001b[39m\"\u001b[39m\u001b[39m response\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m crawl \u001b[39m=\u001b[39m crawler(starting_url\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org/wiki/Kevin_Bacon\u001b[39m\u001b[39m\"\u001b[39m, depth \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m, mongo_uri \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmongodb://localhost:27017\u001b[39m\u001b[39m'\u001b[39m, db_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mM2\u001b[39m\u001b[39m\"\u001b[39m, collection_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBS\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m crawl\u001b[39m.\u001b[39;49mrun_analysis()\n",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m, in \u001b[0;36mcrawler.run_analysis\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m starting_url \u001b[39min\u001b[39;00m tqdm\u001b[39m.\u001b[39mtqdm(hrefs):\n\u001b[0;32m     55\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://en.wikipedia.org\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m starting_url\n\u001b[1;32m---> 56\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39;49mget(url)\n\u001b[0;32m     57\u001b[0m     result \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mcontent    \n\u001b[0;32m     58\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m BeautifulSoup(result, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\requests\\api.py:73\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \n\u001b[0;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39;49m\u001b[39mget\u001b[39;49m\u001b[39m\"\u001b[39;49m, url, params\u001b[39m=\u001b[39;49mparams, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39;49mrequest(method\u001b[39m=\u001b[39;49mmethod, url\u001b[39m=\u001b[39;49murl, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[0;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\kevin\\anaconda3\\envs\\ap\\Lib\\site-packages\\requests\\adapters.py:501\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(\n\u001b[0;32m    487\u001b[0m         method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[0;32m    488\u001b[0m         url\u001b[39m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39mchunked,\n\u001b[0;32m    498\u001b[0m     )\n\u001b[0;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n\u001b[0;32m    503\u001b[0m \u001b[39mexcept\u001b[39;00m MaxRetryError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    504\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e\u001b[39m.\u001b[39mreason, ConnectTimeoutError):\n\u001b[0;32m    505\u001b[0m         \u001b[39m# TODO: Remove this in 3.0.0: see #2811\u001b[39;00m\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "crawl = crawler(starting_url=\"https://en.wikipedia.org/wiki/Kevin_Bacon\", depth = 3, mongo_uri = 'mongodb://localhost:27017', db_name = \"M2\", collection_name=\"BS\")\n",
    "crawl.run_analysis()\n",
    "crawl.Get_hrefs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47d84629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14816"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawl.n_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Scrapy\"></a>\n",
    "## Scrapy\n",
    "\n",
    "Although BS works well on small examples it requires an extra amount of work on larger project to have it well structured. This overhead can be avoided using Scrapy which is another Python webscraping library. Also you can't use Xpaths in BS which are a cleaner way to find elements. The entry cost to scrapy is high but once mastered it will help you a lot in your scraping work. We will try to reproduce the BS wikipedia code but using scrapy. As always installation is straightforward:\n",
    "\n",
    "```console\n",
    "pip install scrapy\n",
    "```\n",
    "\n",
    "Scrapy works by first creating a project. Go to a folder that will have the project inside and run the following in a terminal/cmd prompt:\n",
    "\n",
    "```console\n",
    "scrapy startproject scrapy_wiki\n",
    "```\n",
    "\n",
    "For the moment don't look too much into the folder created, we first want to create a script called \"spider\" in scrapy terminology which will be your main script at the beginning:\n",
    "\n",
    "```console\n",
    "cd scrapy_wiki\n",
    "scrapy genspider spider_wikipedia wikipedia.org\n",
    "```\n",
    "\n",
    "At the end you should have the following structure\n",
    "\n",
    "```\n",
    "scrapy.cfg\n",
    "scrapy_wiki\n",
    "│   \n",
    "└───spiders\n",
    "│   │   __init__.py\n",
    "│   │   spider_wikipedia.py\n",
    "│   __init__.py    \n",
    "│   items.py\n",
    "│   middlewares.py\n",
    "│   pipelines.py\n",
    "│   settings.py\n",
    "```\n",
    "\n",
    "At any point in the process of writing code you can use something called scrapy shell. This allows you to do some small examples and test without having to run the whole thing.\n",
    "\n",
    "```console\n",
    "scrapy shell\n",
    "fetch(\"https://en.wikipedia.org/wiki/Kevin_Bacon\")\n",
    "view(response)\n",
    "hrefs = response.xpath(\"//div[@id='bodyContent']//a[@href[re:test(.,'^(/wiki/)((?!:).)*$')]]/@href\").getall()\n",
    "print(hrefs)\n",
    "```\n",
    "\n",
    "There's a lot to go through so to start let's focus on spider_wikipedia.py, it should look like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_wikipedia.py \n",
    "\n",
    "import scrapy\n",
    "\n",
    "# A class that inherits from scrapy.Spider. We will see in Chap 2 what inheritance is for the moment just know that we \"inherit\" modules from the class scrapy.Spider\n",
    "# This means that you have some function and features already implemented and usable. \n",
    "class SpiderWikipediaSpider(scrapy.Spider):\n",
    "    # the name we introduce during the creation of the spider\n",
    "    name = 'spider_wikipedia'\n",
    "    # If you try to scrap an url outside of allowed_domains it wont work\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    # The first url you will parse\n",
    "    start_urls = ['http://wikipedia.org/']\n",
    "\n",
    "    # What you do with the first url, response = what we get with a request.get()\n",
    "    def parse(self, response):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start small, how do we change this code to get the hrefs and urls and iterate this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_wikipedia.py \n",
    "\n",
    "import scrapy\n",
    "import time\n",
    "# A class that inherits from scrapy.Spider. We will see in CHap 2 what inheritance is for the moment just know that we \"inherit\" modules from the class scrapy.Spider\n",
    "# This means that you have some function and features already implemented and usable. \n",
    "class SpiderWikipediaSpider(scrapy.Spider):\n",
    "    # the name we introduce during the creation of the spider\n",
    "    name = 'spider_wikipedia'\n",
    "    # If you try to scrap an url outside of allowed_domains it wont work\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    # The first url you will parse\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/Kevin_Bacon\"]\n",
    "\n",
    "    # What you do with the first url, response = what we get with a request.get()\n",
    "    def parse(self, response):\n",
    "        # time.sleep because we are nice.\n",
    "        time.sleep(10)\n",
    "        # Nothing new here\n",
    "        hrefs = response.xpath(\"//div[@id='bodyContent']//a[@href[re:test(.,'^(/wiki/)((?!:).)*$')]]/@href\").getall()\n",
    "        full_text = \"\"\n",
    "        for para in response.xpath(\"//p/text()\").getall():\n",
    "            full_text += para + \" \"\n",
    "        \n",
    "        # Scrapy works based on scrapy request, the most important argument being callback\n",
    "        # Basically you get an url and call a function to work on this url (in this case the same as for starting_url: parse())\n",
    "        for url in hrefs:\n",
    "            yield(scrapy.Request(url=\"https://en.wikipedia.org\" + url, callback=self.parse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run it just go the spiders folder and run in a console:\n",
    "\n",
    "```console\n",
    "scrapy runspider spider_wikipedia.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point if you run the spider it won't give you any results and your terminal should look like this:\n",
    "\n",
    "![robots](img/robots.png)\n",
    "\n",
    "Seems like there's some kind of issue with the following url: https://fr.wikipedia.org/robots.txt. \n",
    "Turns out website don't like that robots try to scrap them, basic behavior of scrapy is to respect this rules. Indeed if you look at scrapy_ap/settings.py you'll find the following:\n",
    "\n",
    "```\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "```\n",
    "\n",
    "robots.txt are crucial information but for the sake of the tutorial, and since our goal is not to overflow wikipedia's server, we will turn down this setting to False:\n",
    "\n",
    "```\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "```\n",
    "\n",
    "If you try to run this now it should work although we did not put an ending condition so it will run forever so don't do it ! \n",
    "Before going into more details on the spider let's focus on settings now that we introduced a bit scrapy_ap/settings.py. Indeed there's a lot of commented line and a lot of features you can enable/disable to avoid complex coding scheme. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapy settings for scrapy_wiki project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'scrapy_wiki'\n",
    "\n",
    "SPIDER_MODULES = ['scrapy_wiki.spiders']\n",
    "NEWSPIDER_MODULE = 'scrapy_wiki.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'scrapy_wiki (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'scrapy_wiki.middlewares.ScrapyWikiSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'scrapy_wiki.middlewares.ScrapyWikiDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'scrapy_wiki.pipelines.ScrapyWikiPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But settings are not the only place you can find nice features, you already have seen that allowed_domains avoid getting caught up in weird website, another example is the duplicate ignore feature. Adding dont_filter=True to scrapy.Request() will ignore some of these features. Look there https://doc.scrapy.org/en/latest/topics/request-response.html#request-objects for more scrapy.Request() arguments, pretty sure you'll find some things that are useful for you.\n",
    "\n",
    "Now we talked about settings.py and spiders but there's still a lot of files left, why are they here ? Well as said above it's meant to have a more structured code and not a single file with every operation you do.\n",
    "\n",
    "- items.py is made to handle and restrict the data retrieved from your request.\n",
    "- pipelines.py will process items (clean, saving in mongo, ...).\n",
    "- middlewares.py will process request and response.\n",
    "\n",
    "Let's start with item.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapyWikiItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to create an item that stores the text, an id and the href."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item.py\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class ScrapyWikiItem(scrapy.Item):\n",
    "    # Read the documentation, scrapy.Field() basic item object\n",
    "    id_ = scrapy.Field()\n",
    "    href = scrapy.Field()\n",
    "    text = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then modify spider spider_wikipedia.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_wikipedia.py \n",
    "\n",
    "import scrapy\n",
    "from scrapy_wiki.items import ScrapyWikiItem\n",
    "\n",
    "\n",
    "# A class that inherits from scrapy.Spider. We will see in Chap 2 what inheritance is for the moment just know that we \"inherit\" modules from the class scrapy.Spider\n",
    "# This means that you have some function and features already implemented and usable.\n",
    " \n",
    "class SpiderWikipediaSpider(scrapy.Spider):\n",
    "    # the name we introduce during the creation of the spider\n",
    "    name = 'spider_wikipedia'\n",
    "    # If you try to scrap an url outside of allowed_domains it wont work\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    # The first url you will parse\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/Kevin_Bacon\"]\n",
    "    # create a counter\n",
    "    n_processed = 0\n",
    "\n",
    "    # What you do with the first url, response = what we get with a request.get()\n",
    "    def parse(self, response):\n",
    "\n",
    "        hrefs = response.xpath(\"//div[@id='bodyContent']//a[@href[re:test(.,'^(/wiki/)((?!:).)*$')]]/@href\").getall()\n",
    "        # update counter\n",
    "        self.n_processed += 1\n",
    "        # create instance of item\n",
    "        item = ScrapyWikiItem()\n",
    "        item[\"href\"] = response.url\n",
    "        item[\"text\"] = response.xpath(\"//p/text()\").getall()\n",
    "        item[\"id_\"] = self.n_processed\n",
    "        \n",
    "        # Scrapy works based on scrapy request, the most important argument being callback\n",
    "        # Basically you get an url and call a function to work on this url (in this case the same as for starting_url: parse())\n",
    "        for url in hrefs:\n",
    "            # meta if you want to update item as you go along, in this case not needed\n",
    "            yield scrapy.Request(url=\"https://en.wikipedia.org\" + url, callback=self.parse,meta={'item': item})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we do not process the text for the moment, we will use pipelines.py to do it. For the moment the item is just returned (see below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "class ScrapyWikiPipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to clean the text and put it in a mongodb, we start from a code given in the documentation (https://docs.scrapy.org/en/latest/topics/item-pipeline.html) and just add a clean_text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipelines.py\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "import re\n",
    "import pymongo\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class MongoPipeline:\n",
    "\n",
    "    collection_name = 'scrapy'\n",
    "\n",
    "    def __init__(self, mongo_uri, mongo_db):\n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get('MONGO_URI'),\n",
    "            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')\n",
    "        )\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        self.client.close()\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        item[\"text\"] = self.clean_text(item[\"text\"])\n",
    "        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())\n",
    "        return item\n",
    "    \n",
    "    \n",
    "    def clean_text(self,text):\n",
    "        full_text = re.sub(\"\\n\",\"\",\" \".join(text))\n",
    "        return full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You also need to enable pipelines in the settings and give the DB name and URI in settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings.py\n",
    "\n",
    "ITEM_PIPELINES = {\n",
    "    'scrapy_wiki.pipelines.MongoPipeline': 300,\n",
    "}\n",
    "\n",
    "MONGO_URI = \"mongodb://localhost:27017\"\n",
    "MONGO_DATABASE = \"M2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you need to add a yield to the spider so that it knows to process the item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider_wikipedia.py \n",
    "\n",
    "import scrapy\n",
    "from scrapy_wiki.items import ScrapyWikiItem\n",
    "\n",
    "\n",
    "# A class that inherits from scrapy.Spider. We will see in Chap 2 what inheritance is for the moment just know that we \"inherit\" modules from the class scrapy.Spider\n",
    "# This means that you have some function and features already implemented and usable.\n",
    " \n",
    "class SpiderWikipediaSpider(scrapy.Spider):\n",
    "    # the name we introduce during the creation of the spider\n",
    "    name = 'spider_wikipedia'\n",
    "    # If you try to scrap an url outside of allowed_domains it wont work\n",
    "    allowed_domains = ['wikipedia.org']\n",
    "    # The first url you will parse\n",
    "    start_urls = [\"https://en.wikipedia.org/wiki/Kevin_Bacon\"]\n",
    "    # create a counter\n",
    "    n_processed = 0\n",
    "\n",
    "    # What you do with the first url, response = what we get with a request.get()\n",
    "    def parse(self, response):\n",
    "\n",
    "        hrefs = response.xpath(\"//div[@id='bodyContent']//a[@href[re:test(.,'^(/wiki/)((?!:).)*$')]]/@href\").getall()\n",
    "        # update counter\n",
    "        self.n_processed += 1\n",
    "        # create instance of item\n",
    "        item = ScrapyWikiItem()\n",
    "        item[\"href\"] = response.url\n",
    "        item[\"text\"] = response.xpath(\"//p/text()\").getall()\n",
    "        item[\"id_\"] = self.n_processed\n",
    "        \n",
    "        yield item\n",
    "        # Scrapy works based on scrapy request, the most important argument being callback\n",
    "        # Basically you get an url and call a function to work on this url (in this case the same as for starting_url: parse())\n",
    "        for url in hrefs:\n",
    "            # meta if you want to update item as you go along, in this case not needed\n",
    "            yield scrapy.Request(url=\"https://en.wikipedia.org\" + url, callback=self.parse,meta={'item': item})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it ! You have your first scrapy project ! There's of course much more to see and we still haven't talked about middlewares.py but we stop here for the moment.\n",
    "Ok so now one would think you have all the tools to scrap websites, well think again ! Let's try to see using scrapy shell what you get when scraping twitch for example:\n",
    "\n",
    "\n",
    "```console\n",
    "scrapy shell\n",
    "fetch(\"https://www.twitch.tv/\")\n",
    "view(response)\n",
    "```\n",
    "\n",
    "![twitch](img/twitch.png)\n",
    "\n",
    "\n",
    "Seems like it does not load. This is due to JavaScript. At some point in time the www was only html and css and the scrapping was easier. Today almost every website you use have some javascript runnning in the background making it dynamic. This makes it hard for BS and Scrapy to find what they are looking for. Now comes a new library called Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Selenium\"></a>\n",
    "## Selenium\n",
    "\n",
    "Selenium is meant to act as if a human was using a web browser. This means that you need a web browser for it to work (we will use mozilla but chrome or others are fine too) and a driver (specific for the browser, geckodriver is for mozilla). DL geckodriver here https://github.com/mozilla/geckodriver/releases. Let's start again with twitch. When you start a code with Selenium you should have a page that opens up (default behavior that can be changed), this page is called \"marionette\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The geckodriver version (0.32.0) detected in PATH at C:\\geckodriver\\geckodriver.exe might not be compatible with the detected firefox version (117.0.0.8636); currently, geckodriver 0.33.0 is recommended for firefox 117.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.twitch.tv/directory/category/special-events', 'https://www.twitch.tv/directory/category/special-events', 'https://www.twitch.tv/directory/category/just-chatting', 'https://www.twitch.tv/directory/category/just-chatting', 'https://www.twitch.tv/directory/category/league-of-legends', 'https://www.twitch.tv/directory/category/league-of-legends', 'https://www.twitch.tv/directory/category/starfield', 'https://www.twitch.tv/directory/category/starfield', 'https://www.twitch.tv/directory/category/valorant', 'https://www.twitch.tv/directory/category/valorant', 'https://www.twitch.tv/directory/category/minecraft', 'https://www.twitch.tv/directory/category/minecraft', 'https://www.twitch.tv/directory/category/talk-shows-and-podcasts', 'https://www.twitch.tv/directory/category/talk-shows-and-podcasts', 'https://www.twitch.tv/directory/category/world-of-warcraft', 'https://www.twitch.tv/directory/category/world-of-warcraft', 'https://www.twitch.tv/directory/category/fortnite', 'https://www.twitch.tv/directory/category/fortnite', 'https://www.twitch.tv/directory/category/grand-theft-auto-v', 'https://www.twitch.tv/directory/category/grand-theft-auto-v', 'https://www.twitch.tv/directory/category/codenames', 'https://www.twitch.tv/directory/category/codenames', 'https://www.twitch.tv/directory/category/rocket-league', 'https://www.twitch.tv/directory/category/rocket-league', 'https://www.twitch.tv/directory/category/counter-strike-2', 'https://www.twitch.tv/directory/category/counter-strike-2', 'https://www.twitch.tv/directory/category/fae-farm', 'https://www.twitch.tv/directory/category/fae-farm', 'https://www.twitch.tv/directory/category/sports-1', 'https://www.twitch.tv/directory/category/sports-1', 'https://www.twitch.tv/directory/category/teamfight-tactics', 'https://www.twitch.tv/directory/category/teamfight-tactics', 'https://www.twitch.tv/directory/category/baldurs-gate-3', 'https://www.twitch.tv/directory/category/baldurs-gate-3', 'https://www.twitch.tv/directory/category/trackmania', 'https://www.twitch.tv/directory/category/trackmania', 'https://www.twitch.tv/directory/category/apex-legends', 'https://www.twitch.tv/directory/category/apex-legends', 'https://www.twitch.tv/directory/category/call-of-duty-warzone', 'https://www.twitch.tv/directory/category/call-of-duty-warzone', 'https://www.twitch.tv/directory/category/chants-of-sennaar', 'https://www.twitch.tv/directory/category/chants-of-sennaar', 'https://www.twitch.tv/directory/category/escape-from-tarkov', 'https://www.twitch.tv/directory/category/escape-from-tarkov', 'https://www.twitch.tv/directory/category/tabletop-rpgs', 'https://www.twitch.tv/directory/category/tabletop-rpgs', 'https://www.twitch.tv/directory/category/music', 'https://www.twitch.tv/directory/category/music', 'https://www.twitch.tv/directory/category/dofus', 'https://www.twitch.tv/directory/category/dofus', 'https://www.twitch.tv/directory/category/star-citizen', 'https://www.twitch.tv/directory/category/star-citizen', 'https://www.twitch.tv/directory/category/counter-strike-global-offensive', 'https://www.twitch.tv/directory/category/counter-strike-global-offensive', 'https://www.twitch.tv/directory/category/rust', 'https://www.twitch.tv/directory/category/rust', 'https://www.twitch.tv/directory/category/pools-hot-tubs-and-beaches', 'https://www.twitch.tv/directory/category/pools-hot-tubs-and-beaches', 'https://www.twitch.tv/directory/category/red-dead-redemption-2', 'https://www.twitch.tv/directory/category/red-dead-redemption-2']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Start up the marionette\n",
    "driver = webdriver.Firefox()\n",
    "# go to this page\n",
    "driver.get(\"https://www.twitch.tv/directory\")\n",
    "\n",
    "# Get infos\n",
    "publications_href = driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-16kq0mq-0 kQlOWy tw-link']\")\n",
    "urls = [ref.get_attribute('href') for ref in publications_href]\n",
    "print(urls)\n",
    "# Close marionette\n",
    "#driver.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.twitch.tv/directory/category/special-events', 'https://www.twitch.tv/directory/category/special-events', 'https://www.twitch.tv/directory/category/just-chatting', 'https://www.twitch.tv/directory/category/just-chatting', 'https://www.twitch.tv/directory/category/league-of-legends', 'https://www.twitch.tv/directory/category/league-of-legends', 'https://www.twitch.tv/directory/category/starfield', 'https://www.twitch.tv/directory/category/starfield', 'https://www.twitch.tv/directory/category/valorant', 'https://www.twitch.tv/directory/category/valorant', 'https://www.twitch.tv/directory/category/minecraft', 'https://www.twitch.tv/directory/category/minecraft', 'https://www.twitch.tv/directory/category/talk-shows-and-podcasts', 'https://www.twitch.tv/directory/category/talk-shows-and-podcasts', 'https://www.twitch.tv/directory/category/world-of-warcraft', 'https://www.twitch.tv/directory/category/world-of-warcraft', 'https://www.twitch.tv/directory/category/fortnite', 'https://www.twitch.tv/directory/category/fortnite', 'https://www.twitch.tv/directory/category/grand-theft-auto-v', 'https://www.twitch.tv/directory/category/grand-theft-auto-v', 'https://www.twitch.tv/directory/category/codenames', 'https://www.twitch.tv/directory/category/codenames', 'https://www.twitch.tv/directory/category/rocket-league', 'https://www.twitch.tv/directory/category/rocket-league', 'https://www.twitch.tv/directory/category/counter-strike-2', 'https://www.twitch.tv/directory/category/counter-strike-2', 'https://www.twitch.tv/directory/category/fae-farm', 'https://www.twitch.tv/directory/category/fae-farm', 'https://www.twitch.tv/directory/category/sports-1', 'https://www.twitch.tv/directory/category/sports-1', 'https://www.twitch.tv/directory/category/teamfight-tactics', 'https://www.twitch.tv/directory/category/teamfight-tactics', 'https://www.twitch.tv/directory/category/baldurs-gate-3', 'https://www.twitch.tv/directory/category/baldurs-gate-3', 'https://www.twitch.tv/directory/category/trackmania', 'https://www.twitch.tv/directory/category/trackmania', 'https://www.twitch.tv/directory/category/apex-legends', 'https://www.twitch.tv/directory/category/apex-legends', 'https://www.twitch.tv/directory/category/call-of-duty-warzone', 'https://www.twitch.tv/directory/category/call-of-duty-warzone', 'https://www.twitch.tv/directory/category/chants-of-sennaar', 'https://www.twitch.tv/directory/category/chants-of-sennaar', 'https://www.twitch.tv/directory/category/escape-from-tarkov', 'https://www.twitch.tv/directory/category/escape-from-tarkov', 'https://www.twitch.tv/directory/category/tabletop-rpgs', 'https://www.twitch.tv/directory/category/tabletop-rpgs', 'https://www.twitch.tv/directory/category/music', 'https://www.twitch.tv/directory/category/music', 'https://www.twitch.tv/directory/category/dofus', 'https://www.twitch.tv/directory/category/dofus', 'https://www.twitch.tv/directory/category/star-citizen', 'https://www.twitch.tv/directory/category/star-citizen', 'https://www.twitch.tv/directory/category/counter-strike-global-offensive', 'https://www.twitch.tv/directory/category/counter-strike-global-offensive', 'https://www.twitch.tv/directory/category/rust', 'https://www.twitch.tv/directory/category/rust', 'https://www.twitch.tv/directory/category/pools-hot-tubs-and-beaches', 'https://www.twitch.tv/directory/category/pools-hot-tubs-and-beaches', 'https://www.twitch.tv/directory/category/red-dead-redemption-2', 'https://www.twitch.tv/directory/category/red-dead-redemption-2']\n"
     ]
    }
   ],
   "source": [
    "publications_href = driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-16kq0mq-0 kQlOWy tw-link']\")\n",
    "urls = [ref.get_attribute('href') for ref in publications_href]\n",
    "print(urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this short example you can already see two problems from simulating real behavior:\n",
    "\n",
    "- The loading time\n",
    "- Scrolling to load\n",
    "\n",
    "The loading time can be easily avoided adding a wait condition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The geckodriver version (0.32.0) detected in PATH at C:\\geckodriver\\geckodriver.exe might not be compatible with the detected firefox version (117.0.0.8636); currently, geckodriver 0.33.0 is recommended for firefox 117.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.twitch.tv/directory/category/special-events', 'https://www.twitch.tv/directory/category/special-events', 'https://www.twitch.tv/directory/category/just-chatting', 'https://www.twitch.tv/directory/category/just-chatting', 'https://www.twitch.tv/directory/category/league-of-legends', 'https://www.twitch.tv/directory/category/league-of-legends', 'https://www.twitch.tv/directory/category/starfield', 'https://www.twitch.tv/directory/category/starfield', 'https://www.twitch.tv/directory/category/valorant', 'https://www.twitch.tv/directory/category/valorant', 'https://www.twitch.tv/directory/category/minecraft', 'https://www.twitch.tv/directory/category/minecraft', 'https://www.twitch.tv/directory/category/talk-shows-and-podcasts', 'https://www.twitch.tv/directory/category/talk-shows-and-podcasts', 'https://www.twitch.tv/directory/category/world-of-warcraft', 'https://www.twitch.tv/directory/category/world-of-warcraft', 'https://www.twitch.tv/directory/category/fortnite', 'https://www.twitch.tv/directory/category/fortnite', 'https://www.twitch.tv/directory/category/grand-theft-auto-v', 'https://www.twitch.tv/directory/category/grand-theft-auto-v', 'https://www.twitch.tv/directory/category/codenames', 'https://www.twitch.tv/directory/category/codenames', 'https://www.twitch.tv/directory/category/rocket-league', 'https://www.twitch.tv/directory/category/rocket-league', 'https://www.twitch.tv/directory/category/counter-strike-2', 'https://www.twitch.tv/directory/category/counter-strike-2', 'https://www.twitch.tv/directory/category/fae-farm', 'https://www.twitch.tv/directory/category/fae-farm', 'https://www.twitch.tv/directory/category/sports-1', 'https://www.twitch.tv/directory/category/sports-1', 'https://www.twitch.tv/directory/category/teamfight-tactics', 'https://www.twitch.tv/directory/category/teamfight-tactics', 'https://www.twitch.tv/directory/category/baldurs-gate-3', 'https://www.twitch.tv/directory/category/baldurs-gate-3', 'https://www.twitch.tv/directory/category/trackmania', 'https://www.twitch.tv/directory/category/trackmania', 'https://www.twitch.tv/directory/category/apex-legends', 'https://www.twitch.tv/directory/category/apex-legends', 'https://www.twitch.tv/directory/category/call-of-duty-warzone', 'https://www.twitch.tv/directory/category/call-of-duty-warzone', 'https://www.twitch.tv/directory/category/chants-of-sennaar', 'https://www.twitch.tv/directory/category/chants-of-sennaar', 'https://www.twitch.tv/directory/category/escape-from-tarkov', 'https://www.twitch.tv/directory/category/escape-from-tarkov', 'https://www.twitch.tv/directory/category/tabletop-rpgs', 'https://www.twitch.tv/directory/category/tabletop-rpgs', 'https://www.twitch.tv/directory/category/music', 'https://www.twitch.tv/directory/category/music', 'https://www.twitch.tv/directory/category/dofus', 'https://www.twitch.tv/directory/category/dofus', 'https://www.twitch.tv/directory/category/star-citizen', 'https://www.twitch.tv/directory/category/star-citizen', 'https://www.twitch.tv/directory/category/counter-strike-global-offensive', 'https://www.twitch.tv/directory/category/counter-strike-global-offensive', 'https://www.twitch.tv/directory/category/rust', 'https://www.twitch.tv/directory/category/rust', 'https://www.twitch.tv/directory/category/pools-hot-tubs-and-beaches', 'https://www.twitch.tv/directory/category/pools-hot-tubs-and-beaches', 'https://www.twitch.tv/directory/category/red-dead-redemption-2', 'https://www.twitch.tv/directory/category/red-dead-redemption-2']\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait # New import\n",
    "\n",
    "# Start up the marionette\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# go to this page\n",
    "driver.get(\"https://www.twitch.tv/directory\")\n",
    "# Condition: wait for element, if after 10 second not found then send an error\n",
    "WebDriverWait(driver, 10).until(lambda driver: driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-16kq0mq-0 kQlOWy tw-link']\"))\n",
    "\n",
    "# Get infos\n",
    "publications_href = driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-16kq0mq-0 kQlOWy tw-link']\")\n",
    "urls = [ref.get_attribute('href') for ref in publications_href]\n",
    "print(urls)\n",
    "# Close marionette\n",
    "#driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrolling takes a bit more coding to deal with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The geckodriver version (0.32.0) detected in PATH at C:\\geckodriver\\geckodriver.exe might not be compatible with the detected firefox version (117.0.0.8636); currently, geckodriver 0.33.0 is recommended for firefox 117.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    }
   ],
   "source": [
    "#%% scrolling function example\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "\n",
    "def scrolldown(driver,bottom = False, n = 0 ):\n",
    "    SCROLL_PAUSE_TIME = 2\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if bottom == True:\n",
    "        while True:\n",
    "            # Scroll down to bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "    else:\n",
    "        for i in range(n):            \n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            last_height = new_height\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://twitter.com/ylecun\")\n",
    "scrolldown(driver,bottom=False,n=10)\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this function is pretty general it does not work in some specific case and you need to adapt, improvise and overcome and ActionChains might come in handy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The geckodriver version (0.32.0) detected in PATH at C:\\geckodriver\\geckodriver.exe might not be compatible with the detected firefox version (117.0.0.8636); currently, geckodriver 0.33.0 is recommended for firefox 117.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"9fa3e87b-db54-47be-aaf0-e53a6a0fe5f6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"1f1c9b64-12ae-4fa3-b59c-9f6fcd5ffc13\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"ae209793-5038-459f-9233-092c878f2931\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"d512e9fa-098b-4610-bf32-416a98a9d3de\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"7231760e-cc74-4ac9-89a9-3f55b2ea32c4\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"6160af62-8ce8-42f1-a4ef-422a34be1618\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"fc4bf1a5-f78e-4789-90f4-e90088cdb5b8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"da1c764f-0bf3-4298-bc08-82f238786e15\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"e6cbf60c-9eca-48cf-a3f2-fb782a5c0928\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"95473add-86ed-4c31-9791-b7fe6cda095e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"20b5bb0f-e38e-44ba-8f7b-b23cc488d568\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"469e7dbf-69db-4e22-82cf-116a4f0d3b5d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"44c05c4d-ef8a-4878-a447-1a2998e792e3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"89c7ee9b-58d8-41d0-a554-4305bfc1793e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"4f0f20f8-7f28-4cb3-a7ad-865e20134435\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"a5c52443-f357-4165-9bd8-b4c736e0b637\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"86c5857e-fef0-4111-8fe3-a6d1781b682e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"5f88fc0e-f5c8-47e4-b616-d02a149ec674\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"e9cfa6c7-357d-44db-a638-4d84669f7216\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"42ebf2ae-79ec-4d14-9c33-339566409aeb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"0536717f-8380-4aa2-bc0f-bb19d78533f8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"b6826327-a5e8-461e-89d2-443013314deb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"714d8f13-9053-487e-bcde-456bc14b5d47\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"3406160a-492d-4a8e-a0f6-7017886d02bd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"18bfdf86-54f3-4040-8745-020e4d427ab0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"b6ddf03f-1f7d-4ba3-a7b9-8b32ec60852a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"031feb9d-470c-4434-9e52-038f624721c9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"a484c59a-042e-4b60-b11c-c00e3afcd290\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"9d2663d8-bac1-4f1d-b17d-41e87488d98a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"33f66299-2541-45d3-802b-ac08fb8811b6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"ef0e0cb5-c5db-4283-98c4-a6dcdec7e790\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"684946dc-b888-46f6-8ca9-163ad63a58b1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"e9ffcca1-181f-4085-9f57-2560ed6c2c83\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"698ef670-06ea-4a18-946e-eac52178f183\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"b4a80041-23b4-4458-9403-b006e68d790e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"2f19faca-6fcc-425d-bfd0-675add00b0ad\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"d73a3a65-80a6-4a34-bc0a-7be443d593e5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"713dd42d-110b-4da7-aa92-9c89a989def8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"692377e3-b0f8-4ba9-b6a2-7498951a883c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"f95413f8-b6b0-4b32-aae0-ad260781606e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"9cff4efe-51ad-422b-a2f5-ecd8226c05e5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"0c623cec-7779-4080-bd4a-0d0c4d8b706b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"4bee22c4-1321-42ef-9df2-286d3e973fbd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"e1d6fa2c-f50f-476b-999c-1f6750810200\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"eb048079-2479-4adc-8163-c8f018d8e5ac\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"1b38e72e-7ddf-4fdf-bcfc-4f6b235d573a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"dcf9dc5b-513d-4535-b182-4eac1df7b830\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"8471ce97-5c6c-4e4c-8e0f-a58f41a1b910\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"33605784-36d2-4afa-918a-fafaa79b998b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"3de881d5-254c-4c24-b309-8db4d22a8a76\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"567f0029-d35e-4bd2-a0c0-6389bd93eb5d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"9902168d-d8cf-4a35-99d7-96c6b3ef830a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"29eb18ab-225e-41d7-8419-0dce128c35aa\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"85dbacf6-f042-4fa7-8f84-7e82129306de\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"29e4f86a-3896-4f7e-bc9b-fff586d0d106\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"eee4c3da-316e-4d6b-b7b6-20ed2417291a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"390a40e9-48d5-4867-bf46-01b21b78883f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"42aec28f-ec07-4416-a9bb-3189fad7a856\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"a32c2a66-dc62-4c0f-82ac-3715627065a5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"3f84b52d-b01d-4d8b-afd8-a4c685776ed3\", element=\"818e241a-f68c-4a9c-adb7-490d3a5fdeb0\")>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% this scrolling does not work in all case:\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait    \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.twitch.tv/directory\")\n",
    "WebDriverWait(driver, 10).until(lambda driver: driver.find_elements(By.XPATH, \"//a[@class='ScCoreLink-sc-16kq0mq-0 kQlOWy tw-link']\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# every action chains http://www.allselenium.info/python-selenium-all-mouse-actions-using-actionchains/#clickandhold(onelement=None)\n",
    "# Problem : the bar is longer so you can scroll down the same amount: Hands-on, decay over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scroll_twitch(driver,n=0):\n",
    "    element = driver.find_element(By.XPATH,\"//div[@class='Layout-sc-1xcs6mc-0 dLyLuY']\")\n",
    "    element.click()\n",
    "    for i in range(n):\n",
    "        try:\n",
    "            action_chains = ActionChains(driver)\n",
    "            action_chains.send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "\n",
    "scroll_twitch(driver,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing to see is how to login using Selenium. Some website require authentification to perform certain action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The geckodriver version (0.32.0) detected in PATH at C:\\geckodriver\\geckodriver.exe might not be compatible with the detected firefox version (117.0.0.8636); currently, geckodriver 0.33.0 is recommended for firefox 117.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndriver.add_cookie({'name': 'twitch.lohp.countryCode',\\n  'value': 'GE',\\n  'path': '/',\\n  'domain': '.twitch.tv',\\n  'secure': False,\\n  'httpOnly': False,\\n  'expiry': 1912091189})\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import pickle\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "driver.get(\"https://www.twitch.tv/login\")\n",
    "\n",
    "user_name = \"username\"\n",
    "password = \"password\"\n",
    "\n",
    "element = driver.find_element(By.ID,\"login-username\")\n",
    "element.send_keys(user_name)\n",
    "\n",
    "element = driver.find_element(By.ID,\"password-input\")\n",
    "element.send_keys(password)\n",
    "\n",
    "sign_in = driver.find_element(By.XPATH, \"//button[@data-a-target='passport-login-button']\")\n",
    "sign_in.click()\n",
    "\n",
    "# cookies\n",
    "\n",
    "\n",
    "driver.get_cookies()\n",
    "pickle.dump( driver.get_cookies() , open(\"data/cookies_twitch.pkl\",\"wb\"))\n",
    "\n",
    "cookies = pickle.load(open(\"data/cookies_twitch.pkl\", \"rb\"))\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "    \n",
    "\"\"\"\n",
    "driver.add_cookie({'name': 'twitch.lohp.countryCode',\n",
    "  'value': 'GE',\n",
    "  'path': '/',\n",
    "  'domain': '.twitch.tv',\n",
    "  'secure': False,\n",
    "  'httpOnly': False,\n",
    "  'expiry': 1912091189})\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might have noticed that the structure is similar to BS (+ Xpath).\n",
    "A nice thing could be to have the scrapy structure with a Selenium backend. To do this we will use the middlewares.py of scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#middlewares.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your spider middleware\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "from scrapy import signals\n",
    "\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "class ScrapyWikiSpiderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, dict or Item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Request, dict\n",
    "        # or Item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesn’t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class ScrapyWikiDownloaderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing we want to modify is the process_request() function. Instead of getting the simple response of the request.get(), we will use Selenium to send back a driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#middlewares.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Define here the models for your spider middleware\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "from scrapy import signals\n",
    "from scrapy.http import HtmlResponse\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "class ScrapyWikiSpiderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, dict or Item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Request, dict\n",
    "        # or Item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesn’t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class ScrapyWikiDownloaderMiddleware(object):\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        driver.get(request.url)\n",
    "        body = driver.page_source\n",
    "        return HtmlResponse(driver.current_url, body=body, encoding='utf-8', request=request)\n",
    "    \n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last thing to do enable the middlewares in settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOADER_MIDDLEWARES = {\n",
    "    'scrapy_wiki.middlewares.ScrapyWikiDownloaderMiddleware': 543,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this method is not failproof. Sometimes you'll want to return the driver and not just the response for example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"API\"></a>\n",
    "## APIs\n",
    "\n",
    "We have seen how to scrap information from website directly. Although this seems like a safe methods it is not the best for the server. Most of the time you don't require the whole page but some specific information on this page. Developpers that are ok with you scraping their website have probably implemented some kind of Application Programming Interface (API). This API reduces the overhead of your query and gives you only the data your are interested in. Querying an api is usually as easy as request.get() if the documentation is available. Let's see some small example:\n",
    "\n",
    "(sidenote: API means nothing and everything at the same time. When you are using your phone to send a message you use an API, when you use a library in python you use an API, when you open a webbrowser you use an API... Be wary when you use this acronym !)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'http://arxiv.org/abs/cond-mat/0102536v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/cond-mat/0102536v1', 'updated': '2001-02-28T20:12:09Z', 'updated_parsed': time.struct_time(tm_year=2001, tm_mon=2, tm_mday=28, tm_hour=20, tm_min=12, tm_sec=9, tm_wday=2, tm_yday=59, tm_isdst=0), 'published': '2001-02-28T20:12:09Z', 'published_parsed': time.struct_time(tm_year=2001, tm_mon=2, tm_mday=28, tm_hour=20, tm_min=12, tm_sec=9, tm_wday=2, tm_yday=59, tm_isdst=0), 'title': 'Impact of Electron-Electron Cusp on Configuration Interaction Energies', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Impact of Electron-Electron Cusp on Configuration Interaction Energies'}, 'summary': 'The effect of the electron-electron cusp on the convergence of configuration\\ninteraction (CI) wave functions is examined. By analogy with the\\npseudopotential approach for electron-ion interactions, an effective\\nelectron-electron interaction is developed which closely reproduces the\\nscattering of the Coulomb interaction but is smooth and finite at zero\\nelectron-electron separation. The exact many-electron wave function for this\\nsmooth effective interaction has no cusp at zero electron-electron separation.\\nWe perform CI and quantum Monte Carlo calculations for He and Be atoms, both\\nwith the Coulomb electron-electron interaction and with the smooth effective\\nelectron-electron interaction. We find that convergence of the CI expansion of\\nthe wave function for the smooth electron-electron interaction is not\\nsignificantly improved compared with that for the divergent Coulomb interaction\\nfor energy differences on the order of 1 mHartree. This shows that, contrary to\\npopular belief, description of the electron-electron cusp is not a limiting\\nfactor, to within chemical accuracy, for CI calculations.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The effect of the electron-electron cusp on the convergence of configuration\\ninteraction (CI) wave functions is examined. By analogy with the\\npseudopotential approach for electron-ion interactions, an effective\\nelectron-electron interaction is developed which closely reproduces the\\nscattering of the Coulomb interaction but is smooth and finite at zero\\nelectron-electron separation. The exact many-electron wave function for this\\nsmooth effective interaction has no cusp at zero electron-electron separation.\\nWe perform CI and quantum Monte Carlo calculations for He and Be atoms, both\\nwith the Coulomb electron-electron interaction and with the smooth effective\\nelectron-electron interaction. We find that convergence of the CI expansion of\\nthe wave function for the smooth electron-electron interaction is not\\nsignificantly improved compared with that for the divergent Coulomb interaction\\nfor energy differences on the order of 1 mHartree. This shows that, contrary to\\npopular belief, description of the electron-electron cusp is not a limiting\\nfactor, to within chemical accuracy, for CI calculations.'}, 'authors': [{'name': 'David Prendergast'}, {'name': 'M. Nolan'}, {'name': 'Claudia Filippi'}, {'name': 'Stephen Fahy'}, {'name': 'J. C. Greer'}], 'author_detail': {'name': 'J. C. Greer'}, 'arxiv_affiliation': 'NMRC, University College, Cork, Ireland', 'author': 'J. C. Greer', 'arxiv_doi': '10.1063/1.1383585', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1063/1.1383585', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/cond-mat/0102536v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/cond-mat/0102536v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '11 pages, 6 figures, 3 tables, LaTeX209, submitted to The Journal of\\n  Chemical Physics', 'arxiv_journal_ref': 'J. Chem. Phys. 115, 1626 (2001)', 'arxiv_primary_category': {'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/astro-ph/0608371v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/astro-ph/0608371v1', 'updated': '2006-08-17T14:05:46Z', 'updated_parsed': time.struct_time(tm_year=2006, tm_mon=8, tm_mday=17, tm_hour=14, tm_min=5, tm_sec=46, tm_wday=3, tm_yday=229, tm_isdst=0), 'published': '2006-08-17T14:05:46Z', 'published_parsed': time.struct_time(tm_year=2006, tm_mon=8, tm_mday=17, tm_hour=14, tm_min=5, tm_sec=46, tm_wday=3, tm_yday=229, tm_isdst=0), 'title': 'Electron thermal conductivity owing to collisions between degenerate\\n  electrons', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Electron thermal conductivity owing to collisions between degenerate\\n  electrons'}, 'summary': 'We calculate the thermal conductivity of electrons produced by\\nelectron-electron Coulomb scattering in a strongly degenerate electron gas\\ntaking into account the Landau damping of transverse plasmons. The Landau\\ndamping strongly reduces this conductivity in the domain of ultrarelativistic\\nelectrons at temperatures below the electron plasma temperature. In the inner\\ncrust of a neutron star at temperatures T < 1e7 K this thermal conductivity\\ncompletely dominates over the electron conductivity due to electron-ion\\n(electron-phonon) scattering and becomes competitive with the the electron\\nconductivity due to scattering of electrons by impurity ions.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We calculate the thermal conductivity of electrons produced by\\nelectron-electron Coulomb scattering in a strongly degenerate electron gas\\ntaking into account the Landau damping of transverse plasmons. The Landau\\ndamping strongly reduces this conductivity in the domain of ultrarelativistic\\nelectrons at temperatures below the electron plasma temperature. In the inner\\ncrust of a neutron star at temperatures T < 1e7 K this thermal conductivity\\ncompletely dominates over the electron conductivity due to electron-ion\\n(electron-phonon) scattering and becomes competitive with the the electron\\nconductivity due to scattering of electrons by impurity ions.'}, 'authors': [{'name': 'P. S. Shternin'}, {'name': 'D. G. Yakovlev'}], 'author_detail': {'name': 'D. G. Yakovlev'}, 'arxiv_affiliation': 'Ioffe Physico-Technical Institute', 'author': 'D. G. Yakovlev', 'arxiv_doi': '10.1103/PhysRevD.74.043004', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1103/PhysRevD.74.043004', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/astro-ph/0608371v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/astro-ph/0608371v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '8 pages, 3 figures', 'arxiv_journal_ref': 'Phys.Rev. D74 (2006) 043004', 'arxiv_primary_category': {'term': 'astro-ph', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'astro-ph', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/1802.06593v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/1802.06593v1', 'updated': '2018-02-19T11:51:42Z', 'updated_parsed': time.struct_time(tm_year=2018, tm_mon=2, tm_mday=19, tm_hour=11, tm_min=51, tm_sec=42, tm_wday=0, tm_yday=50, tm_isdst=0), 'published': '2018-02-19T11:51:42Z', 'published_parsed': time.struct_time(tm_year=2018, tm_mon=2, tm_mday=19, tm_hour=11, tm_min=51, tm_sec=42, tm_wday=0, tm_yday=50, tm_isdst=0), 'title': 'Electron pairing: from metastable electron pair to bipolaron', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Electron pairing: from metastable electron pair to bipolaron'}, 'summary': 'Starting from the shell structure in atoms and the significant correlation\\nwithin electron pairs, we distinguish the exchange-correlation effects between\\ntwo electrons of opposite spins occupying the same orbital from the average\\ncorrelation among many electrons in a crystal. In the periodic potential of the\\ncrystal with lattice constant larger than the effective Bohr radius of the\\nvalence electrons, these correlated electron pairs can form a metastable energy\\nband above the corresponding single-electron band separated by an energy gap.\\nIn order to determine if these metastable electron pairs can be stabilized, we\\ncalculate the many-electron exchange-correlation renormalization and the\\npolaron correction to the two-band system with single electrons and electron\\npairs. We find that the electron-phonon interaction is essential to\\ncounterbalance the Coulomb repulsion and to stabilize the electron pairs. The\\ninterplay of the electron-electron and electron-phonon interactions, manifested\\nin the exchange-correlation energies, polaron effects, and screening, is\\nresponsible for the formation of electron pairs (bipolarons) that are located\\non the Fermi surface of the single-electron band.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Starting from the shell structure in atoms and the significant correlation\\nwithin electron pairs, we distinguish the exchange-correlation effects between\\ntwo electrons of opposite spins occupying the same orbital from the average\\ncorrelation among many electrons in a crystal. In the periodic potential of the\\ncrystal with lattice constant larger than the effective Bohr radius of the\\nvalence electrons, these correlated electron pairs can form a metastable energy\\nband above the corresponding single-electron band separated by an energy gap.\\nIn order to determine if these metastable electron pairs can be stabilized, we\\ncalculate the many-electron exchange-correlation renormalization and the\\npolaron correction to the two-band system with single electrons and electron\\npairs. We find that the electron-phonon interaction is essential to\\ncounterbalance the Coulomb repulsion and to stabilize the electron pairs. The\\ninterplay of the electron-electron and electron-phonon interactions, manifested\\nin the exchange-correlation energies, polaron effects, and screening, is\\nresponsible for the formation of electron pairs (bipolarons) that are located\\non the Fermi surface of the single-electron band.'}, 'authors': [{'name': 'Guo-Qiang Hai'}, {'name': 'Ladir Cândido'}, {'name': 'Braulio G. A. Brito'}, {'name': 'François M. Peeters'}], 'author_detail': {'name': 'François M. Peeters'}, 'author': 'François M. Peeters', 'arxiv_doi': '10.1088/2399-6528/aaaee0', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1088/2399-6528/aaaee0', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/1802.06593v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1802.06593v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '17 pages, 6 figures, Journal of Physics Communications 2018', 'arxiv_primary_category': {'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/2010.01066v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2010.01066v1', 'updated': '2020-10-02T15:46:56Z', 'updated_parsed': time.struct_time(tm_year=2020, tm_mon=10, tm_mday=2, tm_hour=15, tm_min=46, tm_sec=56, tm_wday=4, tm_yday=276, tm_isdst=0), 'published': '2020-10-02T15:46:56Z', 'published_parsed': time.struct_time(tm_year=2020, tm_mon=10, tm_mday=2, tm_hour=15, tm_min=46, tm_sec=56, tm_wday=4, tm_yday=276, tm_isdst=0), 'title': 'Electron Temperature Anisotropy and Electron Beam Constraints From\\n  Electron Kinetic Instabilities in the Solar Wind', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Electron Temperature Anisotropy and Electron Beam Constraints From\\n  Electron Kinetic Instabilities in the Solar Wind'}, 'summary': 'Electron temperature anisotropies and electron beams are nonthermal features\\nof the observed nonequilibrium electron velocity distributions in the solar\\nwind. In collision-poor plasmas these nonequilibrium distributions are expected\\nto be regulated by kinetic instabilities through wave-particle interactions.\\nThis study considers electron instabilities driven by the interplay of core\\nelectron temperature anisotropies and the electron beam, and firstly gives a\\ncomprehensive analysis of instabilities in arbitrary directions to the\\nbackground magnetic field. It clarifies the dominant parameter regime (e.g.,\\nparallel core electron plasma beta $\\\\beta_{\\\\mathrm{ec\\\\parallel}}$, core\\nelectron temperature anisotropy $A_{\\\\mathrm{ec}}\\\\equiv\\nT_{\\\\mathrm{ec\\\\perp}}/T_{\\\\mathrm{ec\\\\parallel}}$, and electron beam velocity\\n$V_{\\\\mathrm{eb}}$) for each kind of electron instability (e.g., the electron\\nbeam-driven electron acoustic/magnetoacoustic instability, the electron\\nbeam-driven whistler instability, the electromagnetic electron cyclotron\\ninstability, the electron mirror instability, the electron firehose\\ninstability, and the ordinary-mode instability). It finds that the electron\\nbeam can destabilize electron acoustic/magnetoacoustic waves in the\\nlow-$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$ regime, and whistler waves in the medium-\\nand large-$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$ regime. It also finds that a new\\noblique fast-magnetosonic/whistler instability is driven by the electron beam\\nwith $V_{\\\\mathrm{eb}}\\\\gtrsim7V_{\\\\mathrm{A}}$ in a regime where\\n$\\\\beta_{\\\\mathrm{ec\\\\parallel}}\\\\sim0.1-2$ and $A_{\\\\mathrm{ec}}<1$. Moreover, this\\nstudy presents electromagnetic responses of each kind of electron instability.\\nThese results provide a comprehensive overview for electron instability\\nconstraints on core electron temperature anisotropies and electron beams in the\\nsolar wind.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Electron temperature anisotropies and electron beams are nonthermal features\\nof the observed nonequilibrium electron velocity distributions in the solar\\nwind. In collision-poor plasmas these nonequilibrium distributions are expected\\nto be regulated by kinetic instabilities through wave-particle interactions.\\nThis study considers electron instabilities driven by the interplay of core\\nelectron temperature anisotropies and the electron beam, and firstly gives a\\ncomprehensive analysis of instabilities in arbitrary directions to the\\nbackground magnetic field. It clarifies the dominant parameter regime (e.g.,\\nparallel core electron plasma beta $\\\\beta_{\\\\mathrm{ec\\\\parallel}}$, core\\nelectron temperature anisotropy $A_{\\\\mathrm{ec}}\\\\equiv\\nT_{\\\\mathrm{ec\\\\perp}}/T_{\\\\mathrm{ec\\\\parallel}}$, and electron beam velocity\\n$V_{\\\\mathrm{eb}}$) for each kind of electron instability (e.g., the electron\\nbeam-driven electron acoustic/magnetoacoustic instability, the electron\\nbeam-driven whistler instability, the electromagnetic electron cyclotron\\ninstability, the electron mirror instability, the electron firehose\\ninstability, and the ordinary-mode instability). It finds that the electron\\nbeam can destabilize electron acoustic/magnetoacoustic waves in the\\nlow-$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$ regime, and whistler waves in the medium-\\nand large-$\\\\beta_{\\\\mathrm{ec\\\\parallel}}$ regime. It also finds that a new\\noblique fast-magnetosonic/whistler instability is driven by the electron beam\\nwith $V_{\\\\mathrm{eb}}\\\\gtrsim7V_{\\\\mathrm{A}}$ in a regime where\\n$\\\\beta_{\\\\mathrm{ec\\\\parallel}}\\\\sim0.1-2$ and $A_{\\\\mathrm{ec}}<1$. Moreover, this\\nstudy presents electromagnetic responses of each kind of electron instability.\\nThese results provide a comprehensive overview for electron instability\\nconstraints on core electron temperature anisotropies and electron beams in the\\nsolar wind.'}, 'authors': [{'name': 'Heyu Sun'}, {'name': 'Jinsong Zhao'}, {'name': 'Wen Liu'}, {'name': 'Huasheng Xie'}, {'name': 'Dejin Wu'}], 'author_detail': {'name': 'Dejin Wu'}, 'author': 'Dejin Wu', 'arxiv_doi': '10.3847/1538-4357/abb3ca', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.3847/1538-4357/abb3ca', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2010.01066v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2010.01066v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'physics.space-ph', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'physics.space-ph', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/1501.04914v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/1501.04914v1', 'updated': '2015-01-20T18:48:22Z', 'updated_parsed': time.struct_time(tm_year=2015, tm_mon=1, tm_mday=20, tm_hour=18, tm_min=48, tm_sec=22, tm_wday=1, tm_yday=20, tm_isdst=0), 'published': '2015-01-20T18:48:22Z', 'published_parsed': time.struct_time(tm_year=2015, tm_mon=1, tm_mday=20, tm_hour=18, tm_min=48, tm_sec=22, tm_wday=1, tm_yday=20, tm_isdst=0), 'title': 'Hamiltonian of a many-electron system with single-electron and\\n  electron-pair states in a two-dimensional periodic potential', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Hamiltonian of a many-electron system with single-electron and\\n  electron-pair states in a two-dimensional periodic potential'}, 'summary': 'Based on the metastable electron-pair energy band in a two-dimensional (2D)\\nperiodic potential obtained previously by Hai and Castelano [J. Phys.: Condens.\\nMatter 26, 115502 (2014)], we present in this work a Hamiltonian of many\\nelectrons consisting of single electrons and electron pairs in the 2D system.\\nThe electron-pair states are metastable of energies higher than those of the\\nsingle-electron states at low electron density. We assume two different\\nscenarios for the single-electron band. When it is considered as the lowest\\nconduction band of a crystal, we compare the obtained Hamiltonian with the\\nphenomenological model Hamiltonian of a boson-fermion mixture proposed by\\nFriedberg and Lee [Phys. Rev. B 40, 6745 (1989)]. Single-electron-electron-pair\\nand electron-pair-electron-pair interaction terms appear in our Hamiltonian and\\nthe interaction potentials can be determined from the electron-electron Coulomb\\ninteractions. When we consider the single-electron band as the highest valence\\nband of a crystal, we show that holes in this valence band are important for\\nstabilization of the electron-pair states in the system.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Based on the metastable electron-pair energy band in a two-dimensional (2D)\\nperiodic potential obtained previously by Hai and Castelano [J. Phys.: Condens.\\nMatter 26, 115502 (2014)], we present in this work a Hamiltonian of many\\nelectrons consisting of single electrons and electron pairs in the 2D system.\\nThe electron-pair states are metastable of energies higher than those of the\\nsingle-electron states at low electron density. We assume two different\\nscenarios for the single-electron band. When it is considered as the lowest\\nconduction band of a crystal, we compare the obtained Hamiltonian with the\\nphenomenological model Hamiltonian of a boson-fermion mixture proposed by\\nFriedberg and Lee [Phys. Rev. B 40, 6745 (1989)]. Single-electron-electron-pair\\nand electron-pair-electron-pair interaction terms appear in our Hamiltonian and\\nthe interaction potentials can be determined from the electron-electron Coulomb\\ninteractions. When we consider the single-electron band as the highest valence\\nband of a crystal, we show that holes in this valence band are important for\\nstabilization of the electron-pair states in the system.'}, 'authors': [{'name': 'G. -Q. Hai'}, {'name': 'F. M. Peeters'}], 'author_detail': {'name': 'F. M. Peeters'}, 'author': 'F. M. Peeters', 'arxiv_doi': '10.1140/epjb/e2014-50686-x', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1140/epjb/e2014-50686-x', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/1501.04914v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/1501.04914v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_journal_ref': 'Eur. Phys. J. B (2015) 88: 20', 'arxiv_primary_category': {'term': 'cond-mat.supr-con', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cond-mat.supr-con', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cond-mat.mes-hall', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/0707.4225v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/0707.4225v1', 'updated': '2007-07-28T09:32:22Z', 'updated_parsed': time.struct_time(tm_year=2007, tm_mon=7, tm_mday=28, tm_hour=9, tm_min=32, tm_sec=22, tm_wday=5, tm_yday=209, tm_isdst=0), 'published': '2007-07-28T09:32:22Z', 'published_parsed': time.struct_time(tm_year=2007, tm_mon=7, tm_mday=28, tm_hour=9, tm_min=32, tm_sec=22, tm_wday=5, tm_yday=209, tm_isdst=0), 'title': 'Electron-Electron Bremsstrahlung Emission and the Inference of Electron\\n  Flux Spectra in Solar Flares', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Electron-Electron Bremsstrahlung Emission and the Inference of Electron\\n  Flux Spectra in Solar Flares'}, 'summary': 'Although both electron-ion and electron-electron bremsstrahlung contribute to\\nthe hard X-ray emission from solar flares, the latter is normally ignored. Such\\nan omission is not justified at electron (and photon) energies above $\\\\sim 300$\\nkeV, and inclusion of the additional electron-electron bremsstrahlung in\\ngeneral makes the electron spectrum required to produce a given hard X-ray\\nspectrum steeper at high energies.\\n  Unlike electron-ion bremsstrahlung, electron-electron bremsstrahlung cannot\\nproduce photons of all energies up to the maximum electron energy involved. The\\nmaximum possible photon energy depends on the angle between the direction of\\nthe emitting electron and the emitted photon, and this suggests a diagnostic\\nfor an upper cutoff energy and/or for the degree of beaming of the accelerated\\nelectrons.\\n  We analyze the large event of January 17, 2005 observed by RHESSI and show\\nthat the upward break around 400 keV in the observed hard X-ray spectrum is\\nnaturally accounted for by the inclusion of electron-electron bremsstrahlung.\\nIndeed, the mean source electron spectrum recovered through a regularized\\ninversion of the hard X-ray spectrum, using a cross-section that includes both\\nelectron-ion and electron-electron terms, has a relatively constant spectral\\nindex $\\\\delta$ over the range from electron kinetic energy $E = 200$ keV to $E\\n= 1$ MeV. However, the level of detail discernible in the recovered electron\\nspectrum is not sufficient to determine whether or not any upper cutoff energy\\nexists.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Although both electron-ion and electron-electron bremsstrahlung contribute to\\nthe hard X-ray emission from solar flares, the latter is normally ignored. Such\\nan omission is not justified at electron (and photon) energies above $\\\\sim 300$\\nkeV, and inclusion of the additional electron-electron bremsstrahlung in\\ngeneral makes the electron spectrum required to produce a given hard X-ray\\nspectrum steeper at high energies.\\n  Unlike electron-ion bremsstrahlung, electron-electron bremsstrahlung cannot\\nproduce photons of all energies up to the maximum electron energy involved. The\\nmaximum possible photon energy depends on the angle between the direction of\\nthe emitting electron and the emitted photon, and this suggests a diagnostic\\nfor an upper cutoff energy and/or for the degree of beaming of the accelerated\\nelectrons.\\n  We analyze the large event of January 17, 2005 observed by RHESSI and show\\nthat the upward break around 400 keV in the observed hard X-ray spectrum is\\nnaturally accounted for by the inclusion of electron-electron bremsstrahlung.\\nIndeed, the mean source electron spectrum recovered through a regularized\\ninversion of the hard X-ray spectrum, using a cross-section that includes both\\nelectron-ion and electron-electron terms, has a relatively constant spectral\\nindex $\\\\delta$ over the range from electron kinetic energy $E = 200$ keV to $E\\n= 1$ MeV. However, the level of detail discernible in the recovered electron\\nspectrum is not sufficient to determine whether or not any upper cutoff energy\\nexists.'}, 'authors': [{'name': 'Eduard P. Kontar'}, {'name': 'A. Gordon Emslie'}, {'name': 'Anna Maria Massone'}, {'name': 'Michele Piana'}, {'name': 'John C. Brown'}, {'name': 'Marco Prato'}], 'author_detail': {'name': 'Marco Prato'}, 'author': 'Marco Prato', 'arxiv_doi': '10.1086/521977', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1086/521977', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/0707.4225v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/0707.4225v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '7 pages, 5 figures, submitted to Astrophysical Journal', 'arxiv_primary_category': {'term': 'astro-ph', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'astro-ph', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/astro-ph/9904306v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/astro-ph/9904306v1', 'updated': '1999-04-22T15:54:59Z', 'updated_parsed': time.struct_time(tm_year=1999, tm_mon=4, tm_mday=22, tm_hour=15, tm_min=54, tm_sec=59, tm_wday=3, tm_yday=112, tm_isdst=0), 'published': '1999-04-22T15:54:59Z', 'published_parsed': time.struct_time(tm_year=1999, tm_mon=4, tm_mday=22, tm_hour=15, tm_min=54, tm_sec=59, tm_wday=3, tm_yday=112, tm_isdst=0), 'title': 'Improved scenario of baryogenesis', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Improved scenario of baryogenesis'}, 'summary': 'It is assumed that, in the primordial plasma, at the temperatures above the\\nmass of electron, fermions are in the neutral state being the superposition of\\nparticle and antiparticle. There exists neutral proton-electron symmetry.\\nProton-electron equilibrium is defined by the proton-electron mass difference.\\nAt the temperature equal to the mass of electron, pairs of neutral electrons\\nannihilate into photons, and pairs of neutral protons and electrons survive as\\nprotons and electrons.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'It is assumed that, in the primordial plasma, at the temperatures above the\\nmass of electron, fermions are in the neutral state being the superposition of\\nparticle and antiparticle. There exists neutral proton-electron symmetry.\\nProton-electron equilibrium is defined by the proton-electron mass difference.\\nAt the temperature equal to the mass of electron, pairs of neutral electrons\\nannihilate into photons, and pairs of neutral protons and electrons survive as\\nprotons and electrons.'}, 'authors': [{'name': 'D. L. Khokhlov'}], 'author_detail': {'name': 'D. L. Khokhlov'}, 'author': 'D. L. Khokhlov', 'arxiv_comment': '3 pages LaTeX', 'links': [{'href': 'http://arxiv.org/abs/astro-ph/9904306v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/astro-ph/9904306v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'astro-ph', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'astro-ph', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/cond-mat/0310615v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/cond-mat/0310615v1', 'updated': '2003-10-27T08:59:02Z', 'updated_parsed': time.struct_time(tm_year=2003, tm_mon=10, tm_mday=27, tm_hour=8, tm_min=59, tm_sec=2, tm_wday=0, tm_yday=300, tm_isdst=0), 'published': '2003-10-27T08:59:02Z', 'published_parsed': time.struct_time(tm_year=2003, tm_mon=10, tm_mday=27, tm_hour=8, tm_min=59, tm_sec=2, tm_wday=0, tm_yday=300, tm_isdst=0), 'title': 'Exact Electron-Pairing Ground States of Tight-Binding Models with Local\\n  Attractive Interactions', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Exact Electron-Pairing Ground States of Tight-Binding Models with Local\\n  Attractive Interactions'}, 'summary': 'We present a class of exactly solvable models of correlated electrons. The\\nmodels are defined in any dimension $d$ and consist of electron-hopping terms\\nand local attractive interactions between electrons. For each even number of\\nelectrons less than or equal to $1/(d+1)$-filling, we find the exact ground\\nstate in which all electrons form pairs of a certain type, and thus the models\\nexhibit an electron-pair condensation.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We present a class of exactly solvable models of correlated electrons. The\\nmodels are defined in any dimension $d$ and consist of electron-hopping terms\\nand local attractive interactions between electrons. For each even number of\\nelectrons less than or equal to $1/(d+1)$-filling, we find the exact ground\\nstate in which all electrons form pairs of a certain type, and thus the models\\nexhibit an electron-pair condensation.'}, 'authors': [{'name': 'Akinori Tanaka'}], 'author_detail': {'name': 'Akinori Tanaka'}, 'author': 'Akinori Tanaka', 'arxiv_doi': '10.1143/JPSJ.73.1107', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1143/JPSJ.73.1107', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/cond-mat/0310615v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/cond-mat/0310615v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': '4 pages, 1 figure', 'arxiv_primary_category': {'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/2101.10508v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2101.10508v1', 'updated': '2021-01-26T01:15:10Z', 'updated_parsed': time.struct_time(tm_year=2021, tm_mon=1, tm_mday=26, tm_hour=1, tm_min=15, tm_sec=10, tm_wday=1, tm_yday=26, tm_isdst=0), 'published': '2021-01-26T01:15:10Z', 'published_parsed': time.struct_time(tm_year=2021, tm_mon=1, tm_mday=26, tm_hour=1, tm_min=15, tm_sec=10, tm_wday=1, tm_yday=26, tm_isdst=0), 'title': 'Insights into the Electron-Electron Interaction from Quantum Monte Carlo\\n  Calculations', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Insights into the Electron-Electron Interaction from Quantum Monte Carlo\\n  Calculations'}, 'summary': 'The effective electron-electron interaction in the electron gas depends on\\nboth the density and spin local field factors. Variational Diagrammatic Quantum\\nMonte Carlo calculations of the spin local field factor are reported and used\\nto quantitatively present the full spin-dependent, electron-electron\\ninteraction. Together with the charge local field factor from previous\\nDiffusion Quantum Monte Carlo calculations, we obtain the complete form of the\\neffective electron-electron interaction in the uniform three-dimensional\\nelectron gas. Very simple quadratic formulas are presented for the local field\\nfactors that quantitatively produce all of the response functions of the\\nelectron gas at metallic densities.\\n  Exchange and correlation become increasingly important at low densities. At\\nthe compressibility divergence at rs = 5.25, both the direct (screened Coulomb)\\nterm and the charge-dependent exchange term in the electron-electron\\ninteraction at q=0 are separately divergent. However, due to large\\ncancellations, their difference is finite, well behaved, and much smaller than\\neither term separately. As a result, the spin contribution to the\\nelectron-electron interaction becomes an important factor. The static\\nelectron-electron interaction is repulsive as a function of density but is less\\nrepulsive for electrons with parallel spins.\\n  The effect of allowing a deformable, rather than rigid, positive background\\nis shown to be as quantitatively important as exchange and correlation. As a\\nsimple concrete example, the electron-electron interaction is calculated using\\nthe measured bulk modulus of the alkali metals with a linear phonon dispersion.\\nThe net electron-electron interaction in lithium is attractive for wave vectors\\n$0-2k_F$, which suggests superconductivity, and is mostly repulsive for the\\nother alkali metals.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'The effective electron-electron interaction in the electron gas depends on\\nboth the density and spin local field factors. Variational Diagrammatic Quantum\\nMonte Carlo calculations of the spin local field factor are reported and used\\nto quantitatively present the full spin-dependent, electron-electron\\ninteraction. Together with the charge local field factor from previous\\nDiffusion Quantum Monte Carlo calculations, we obtain the complete form of the\\neffective electron-electron interaction in the uniform three-dimensional\\nelectron gas. Very simple quadratic formulas are presented for the local field\\nfactors that quantitatively produce all of the response functions of the\\nelectron gas at metallic densities.\\n  Exchange and correlation become increasingly important at low densities. At\\nthe compressibility divergence at rs = 5.25, both the direct (screened Coulomb)\\nterm and the charge-dependent exchange term in the electron-electron\\ninteraction at q=0 are separately divergent. However, due to large\\ncancellations, their difference is finite, well behaved, and much smaller than\\neither term separately. As a result, the spin contribution to the\\nelectron-electron interaction becomes an important factor. The static\\nelectron-electron interaction is repulsive as a function of density but is less\\nrepulsive for electrons with parallel spins.\\n  The effect of allowing a deformable, rather than rigid, positive background\\nis shown to be as quantitatively important as exchange and correlation. As a\\nsimple concrete example, the electron-electron interaction is calculated using\\nthe measured bulk modulus of the alkali metals with a linear phonon dispersion.\\nThe net electron-electron interaction in lithium is attractive for wave vectors\\n$0-2k_F$, which suggests superconductivity, and is mostly repulsive for the\\nother alkali metals.'}, 'authors': [{'name': 'Carl A. Kukkonen'}, {'name': 'Kun Chen'}], 'author_detail': {'name': 'Kun Chen'}, 'author': 'Kun Chen', 'arxiv_comment': '16 pages, 20 figures', 'links': [{'href': 'http://arxiv.org/abs/2101.10508v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2101.10508v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cond-mat.quant-gas', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cond-mat.quant-gas', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cond-mat.mtrl-sci', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}, {'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "{'id': 'http://arxiv.org/abs/cond-mat/0205001v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/cond-mat/0205001v1', 'updated': '2002-04-30T20:00:18Z', 'updated_parsed': time.struct_time(tm_year=2002, tm_mon=4, tm_mday=30, tm_hour=20, tm_min=0, tm_sec=18, tm_wday=1, tm_yday=120, tm_isdst=0), 'published': '2002-04-30T20:00:18Z', 'published_parsed': time.struct_time(tm_year=2002, tm_mon=4, tm_mday=30, tm_hour=20, tm_min=0, tm_sec=18, tm_wday=1, tm_yday=120, tm_isdst=0), 'title': 'Electron-electron interactions in a weakly screened two-dimensional\\n  electron system', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Electron-electron interactions in a weakly screened two-dimensional\\n  electron system'}, 'summary': 'We probe the strength of electron-electron interactions using\\nmagnetoconductivity measurements of two-dimensional non-degenerate electrons on\\nliquid helium at 1.22 K. Our data extend to electron densities that are two\\norders of magnitude smaller than previously reported. We span both the\\nindependent-electron regime where the data are qualitatively described by the\\nself-consistent Born approximation (SCBA), and the strongly-interacting\\nelectron regime. At finite fields we observe a crossover from SCBA to Drude\\ntheory as a function of electron density.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'We probe the strength of electron-electron interactions using\\nmagnetoconductivity measurements of two-dimensional non-degenerate electrons on\\nliquid helium at 1.22 K. Our data extend to electron densities that are two\\norders of magnitude smaller than previously reported. We span both the\\nindependent-electron regime where the data are qualitatively described by the\\nself-consistent Born approximation (SCBA), and the strongly-interacting\\nelectron regime. At finite fields we observe a crossover from SCBA to Drude\\ntheory as a function of electron density.'}, 'authors': [{'name': 'I. Karakurt'}, {'name': 'A. J. Dahm'}], 'author_detail': {'name': 'A. J. Dahm'}, 'author': 'A. J. Dahm', 'arxiv_comment': '4 pages, 5 figures', 'links': [{'href': 'http://arxiv.org/abs/cond-mat/0205001v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/cond-mat/0205001v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_primary_category': {'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cond-mat.str-el', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n"
     ]
    }
   ],
   "source": [
    "#%% Arxiv\n",
    "\n",
    "# First objective find if there's an API and the documentation of the API\n",
    "# https://arxiv.org/help/api/tou\n",
    "# https://arxiv.org/help/api/basics\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "\n",
    "response = requests.get('http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=10')\n",
    "feed = feedparser.parse(response.content)\n",
    "feed\n",
    "\n",
    "results = {}\n",
    "for entry in feed.entries:\n",
    "    print(entry)\n",
    "    results[entry.id] = {\"title\": entry.title,\n",
    "                         \"abstract\":entry.summary}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the api you'll have to work with different data format. The most popular is json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Boston'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "response = requests.get(\"http://ip-api.com/json/50.78.253.58\")\n",
    "response.content\n",
    "json_str = json.loads(response.content)\n",
    "json_str[\"city\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other less popular format is xml. The next code will show you how to parse it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'J. Asian Earth Sci.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Archaeometry'),\n",
       " ('source', 'Archaeometry'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Geochronometria'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Nature'),\n",
       " ('source', 'Boreas'),\n",
       " ('source', 'Boreas'),\n",
       " ('source', 'Archaeometry'),\n",
       " ('source', 'Thermoluminescence Dating'),\n",
       " ('source', 'Geochronometria'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Boreas'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Sci. Rev.'),\n",
       " ('source', 'Quat. Sci. Rev.'),\n",
       " ('source',\n",
       "  'Geologia Cuaternarului: Notiuni de Baza; (Quaternary Geology. Basic Notions)'),\n",
       " ('source', 'Bulletin de l’Association Française Pour L’étude du Quaternaire'),\n",
       " ('source',\n",
       "  'Formatiuni Cuaternareîn Dobrogea (Loessuri si Paleosoluri) (Quaternary Units in Dobrogea)'),\n",
       " ('source', 'Phys. Chem. Earth A'),\n",
       " ('source', 'Rom. Rep. Phys.'),\n",
       " ('source', 'Quaternaire'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Anu. Inst. Geol. României'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Quat. Sci. Rev.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Catena'),\n",
       " ('source', 'Rom. Rep. Phys.'),\n",
       " ('source', 'Quat. Int'),\n",
       " ('source', 'Earth Planet. Sci. Lett.'),\n",
       " ('source', 'Geology'),\n",
       " ('source', 'Paleoceanography'),\n",
       " ('source', 'Paleoceanography'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Quat. Int.'),\n",
       " ('source', 'Quat. Sci. Rev.'),\n",
       " ('source', 'Anc. TL'),\n",
       " ('source', 'Anc. TL'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Anc. TL'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Quat. Geochronol.'),\n",
       " ('source', 'Radiat. Meas.'),\n",
       " ('source', 'Radiat. Meas.')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% XML\n",
    "# XML = a common language when doing requests. Extensible Markup Language. tree-like structure.\n",
    "# Multiple package to work with python and xml: lxml, xml.dom.minidom, xml.etree.ElementTree\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "\n",
    "xml_file = \"data/Chap1/mps-03-00019.nxml\"\n",
    "\n",
    "# ET\n",
    "tree = ET.parse(xml_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "[(elem.tag, elem.text) for elem in root.iter()]\n",
    "[(elem.tag, elem.text) for elem in root.iter() if elem.tag ==\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The loess-paleosol archive from Mircea Vodă (Romania) represents one of the most studied sections in Europe. We are applying here the current state of the art luminescence dating protocols for revisiting the chronology of this section. Analysis were performed on fine (4–11 µm) and coarse (63–90 µm) quartz extracts using the single aliquot regenerative (SAR) optically stimulated luminescence (OSL) dating protocol. Laboratory generated SAR dose response curves in the high dose range (5 kGy for fine quartz and 2 kGy for coarse quartz) were investigated by employing a test dose of either 17 or 170 Gy. The results confirm the previously reported different saturation characteristics of the two quartz fractions, with no evident dependency of the equivalent dose (D'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# minidom\n",
    "doc = xml.dom.minidom.parse(xml_file) \n",
    "\n",
    "abstract = doc.getElementsByTagName(\"abstract\")\n",
    "body = doc.getElementsByTagName(\"body\")\n",
    "title = doc.getElementsByTagName(\"title-group\")\n",
    "figures = doc.getElementsByTagName(\"fig\")\n",
    "\n",
    "abstract[0].childNodes[0].childNodes[0].nodeValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timar-Gabor\n"
     ]
    }
   ],
   "source": [
    "#lxml\n",
    "\n",
    "from lxml import etree\n",
    "\n",
    "root = etree.parse(xml_file)\n",
    "abstract = root.xpath(\"//abstract//text()\")\n",
    "body = root.xpath(\"//body//text()\")\n",
    "title = root.xpath(\"//title-group//text()\")\n",
    "figures = root.xpath(\"//fig//text()\")\n",
    "\n",
    "aff = root.xpath(\"//aff/text()\")\n",
    "aff = [i for i in aff if not i.startswith((' ', '\\t'))]\n",
    "aff_label = root.xpath(\"//aff/label/text()\")\n",
    "\n",
    "mails =root.xpath(\"//author-notes/corresp\")[0]\n",
    "mails.getchildren()\n",
    "\n",
    "xref = {}\n",
    "for affiliation,label in zip(aff,aff_label):\n",
    "    xref[label]= affiliation\n",
    "\n",
    "authors = root.xpath(\"//contrib\")\n",
    "authors = [i.getchildren() for i in authors]\n",
    "for author in authors:\n",
    "    names = [i.getchildren() for i in author if i.tag == \"name\"][0]\n",
    "    surname = [i.text for i in names if i.tag==\"surname\"]\n",
    "    name = [i.text for i in names if i.tag==\"given-names\"]\n",
    "    xrefs = [i.text for i in author if i.tag==\"xref\"]\n",
    "\n",
    "    \n",
    "print(names[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Twitch\n",
    "\n",
    "# https://dev.twitch.tv/docs/authentication\n",
    "# https://dev.twitch.tv/docs/api\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pymongo\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "client = pymongo.MongoClient('localhost',27017)\n",
    "\n",
    "mydb = client[\"api_twitch\"]\n",
    "collection = mydb[\"top_games\"]\n",
    "\n",
    "\n",
    "Client_ID = \"zuxz59ow9v8zncx3ljdyo5jaj1sqdz\"\n",
    "secret = \"julw385uytn4dhzkublz2wa644l3he\"\n",
    "\n",
    "access_token = requests.post(\"https://id.twitch.tv/oauth2/token?client_id={}&client_secret={}&grant_type=client_credentials\".format(Client_ID,secret))\n",
    "access_token = json.loads(access_token.content)[\"access_token\"]\n",
    "#scope = \"analytics:read:games\"\n",
    "headers = {\"Client-ID\": Client_ID, \"Authorization\": \"Bearer \" + access_token,}\n",
    "\n",
    "n_games = 40\n",
    "limit = 20\n",
    "n_iteration = int(n_games/limit)\n",
    "\n",
    "for i in tqdm.tqdm(range(n_iteration)):\n",
    "    if i == 0:\n",
    "        response_category = requests.get(\"https://api.twitch.tv/helix/games/top\",headers = headers)\n",
    "    else:\n",
    "        response_category = requests.get(\"https://api.twitch.tv/helix/games/top?after={}\".format(json.loads(response_category.content)[\"pagination\"][\"cursor\"]),headers = headers)\n",
    "    for category in json.loads(response_category.content)[\"data\"]:\n",
    "        response = requests.get('https://api.twitch.tv/helix/streams?game_id={}'.format(category[\"id\"]), headers=headers)\n",
    "        streamers = {}\n",
    "        for streamer in json.loads(response.content)[\"data\"]:\n",
    "            streamers[streamer[\"user_id\"]] = {\"user_name\":streamer[\"user_name\"],\n",
    "                                              \"title\":streamer[\"title\"],\n",
    "                                              \"viewer_count\":streamer[\"viewer_count\"],\n",
    "                                              \"started_at\":streamer[\"started_at\"],\n",
    "                                              \"language\":streamer[\"language\"],} \n",
    "        \n",
    "        done = False\n",
    "        while done == False:\n",
    "            try:\n",
    "                response = requests.get('https://api.twitch.tv/helix/streams?game_id={}&after={}'.format(category[\"id\"],json.loads(response.content)[\"pagination\"][\"cursor\"]), headers=headers)\n",
    "                for streamer in json.loads(response.content)[\"data\"]:\n",
    "                    streamers[streamer[\"user_id\"]] = {\"user_name\":streamer[\"user_name\"],\n",
    "                                                      \"title\":streamer[\"title\"],\n",
    "                                                      \"viewer_count\":streamer[\"viewer_count\"],\n",
    "                                                      \"started_at\":streamer[\"started_at\"],\n",
    "                                                      \"language\":streamer[\"language\"],}\n",
    "                time.sleep(1)\n",
    "            except:\n",
    "                done = True\n",
    "        \n",
    "        post = {\"_id\": category[\"id\"],\n",
    "                \"game\": category[\"name\"],\n",
    "                \"streamers\": streamers,\n",
    "                }\n",
    "        try:\n",
    "            collection.insert_one(post)\n",
    "        except Exception as e:\n",
    "            print(str(e))\n",
    "            \n",
    "    print(response.headers)\n",
    "    time.sleep(1)\n",
    "    \n",
    "\n",
    "\n",
    "cursor = collection.find({\"game\":\"World of Warcraft\"})\n",
    "for document in cursor:\n",
    "    print(document)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"FTP\"></a>\n",
    "## FTP server\n",
    "\n",
    "Sometimes website don't have APIs but FTP server where you can download bulk of data. Its unlikely that you'll encounter FTP server soon but just in case here is how you do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'221 Goodbye.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ftplib\n",
    "import re\n",
    "import tarfile\n",
    "import os\n",
    "#import shutil\n",
    "\n",
    "# callback for ftp.retrbinary\n",
    "def file_write(data):\n",
    "   local_file.write(data) \n",
    "\n",
    "# connect to ftp_server\n",
    "email = \"kevin.wirtz@unistra.fr\"\n",
    "ftp = ftplib.FTP(\"ftp.ncbi.nlm.nih.gov\")\n",
    "ftp.login(user=\"anonymous\",passwd=email)\n",
    "ftp.cwd(\"pub/pmc/oa_package\")\n",
    "\n",
    "# Find tar.gz path\n",
    "\n",
    "tar_gz = False\n",
    "while tar_gz == False:\n",
    "    list_files = ftp.nlst()\n",
    "    if re.search(\"\\.\",list_files[-1]):\n",
    "        tar_gz = True\n",
    "    else:\n",
    "        ftp.cwd(list_files[-1])\n",
    "\n",
    "# In this case we will only take 1 file, dl it and uncompress it\n",
    "\n",
    "link = ftp.pwd() +\"/\" + list_files[0]\n",
    "path = 'data/Chap1/{}'.format(list_files[0])\n",
    "local_file = open(path,\"wb\")\n",
    "ftp.retrbinary(\"RETR \" + link,file_write, blocksize=16384)\n",
    "local_file.close()\n",
    "my_tar = tarfile.open(path)\n",
    "my_tar.extractall(\"data/Chap1/\")\n",
    "my_tar.close()\n",
    "os.unlink(path)\n",
    "\n",
    "ftp.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"rules\"></a>\n",
    "## Rules of good conduct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping is a gray area. Some legal action examples:\n",
    "- eBay vs Bidder’s Edge (Too much scraping)\n",
    "- US vs Auernheimer (Taking advantage of a security breach)\n",
    "- Field vs Google (Google scraping and caching a website)\n",
    "\n",
    "Best way to avoid problems:\n",
    "- Follow the rules (e.g:  robots.txt, ToS).\n",
    "- Don’t overload the server (Avoid parallelism and put sometime.sleep()).\n",
    "- Scrap when there’s less users (night).\n",
    "- Contact the authors of the website to let them know what you aredoing.\n",
    "- Use an API if there’s one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "Code review:\n",
    "- https://github.com/matthpn2/Web-Scraping-with-Beautiful-Soup\n",
    "- https://github.com/SoumitraAgarwal/Fifa-Ratings\n",
    "- https://github.com/RainrainWu/finance_scraper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
