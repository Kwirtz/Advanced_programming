{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: MISC.\n",
    "\n",
    "The goal of this chapter is to gain insights in multiple things that you might encounter in your work but also that you will probably learn more about in other course.\n",
    "\n",
    "I already talked about the way I do my work by combining both Python and R. Other Business Analytics tools exists and are popular in companies. I really suggest to look into it and try to integrate them in your workflow. To get started with the different softwares and see how it works we will use data from the UCI Machine Learning repository https://archive.ics.uci.edu/dataset/116/us+census+data+1990.\n",
    "\n",
    "- [Pytorch and Keras](#Pytorch)\n",
    "- [Power BI](#PowerBI)\n",
    "- [Previous Project](#PreviousProject)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f74816",
   "metadata": {},
   "source": [
    "<a name=\"Pytorch\"></a>\n",
    "## Pytorch and Keras\n",
    "\n",
    "This section will introduce two of the prominent machine learning library: Pytorch and Keras.\n",
    "\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab (FAIR). It has gained popularity for its flexibility, dynamic computational graph, and ease of use. PyTorch is widely used for deep learning applications, including neural network development, natural language processing, computer vision, and more. At the core of PyTorch is the tensor, which is a multi-dimensional array similar to NumPy arrays. Tensors can represent scalars, vectors, matrices, and higher-dimensional arrays. PyTorch provides a variety of tensor operations that make it easy to perform mathematical computations.\n",
    "\n",
    "Keras is a high-level neural networks API that provides a user-friendly interface for building and training deep learning models. It abstracts away many low-level details, making it easy for beginners to create and experiment with neural networks. It is well-integrated with popular deep learning backends, especially TensorFlow, and offers flexibility in model design through both Sequential and Functional model APIs. Keras seamlessly works with NumPy arrays. You typically preprocess your data using NumPy, and Keras models accept NumPy arrays as input. You create the model architecture, compile it, and then feed data through the model. Known for its simplicity and user-friendly design, Keras provides a high-level API that abstracts many details, making it accessible for beginners.\n",
    "\n",
    "PyTorch uses its own data structure called tensors. While it's possible to convert between NumPy arrays and PyTorch tensors, PyTorch tensors are the native data type. PyTorch uses dynamic computational graphs. This means the graph is built on-the-fly as operations are executed. This can be advantageous in certain scenarios, especially for dynamic and varying input sizes. PyTorch is designed to be more explicit, giving users more control over the details of their models. This can be advantageous for researchers and developers who need flexibility.\n",
    "\n",
    "PyTorch is often used in research-oriented environments, while Keras is more commonly applied in practical settings. Generally, if you want to become a data scientist, it's recommended to learn PyTorch. On the other hand, if you aim to become a data analyst or engineer, learning Keras can be beneficial. Of course this is my personal opinion and not the absolute truth.\n",
    "\n",
    "In the rest of this section, we will first start to get familiar with both of the libraries. We then demonstrate various examples of neural networks (NN), convolutional neural networks (CNN), and graph neural networks (GNN) using both Keras and PyTorch. Through these examples, you will gain insights into the advantages of each framework and be better able to determine which one aligns more with your preferences. Of course, this is not intended to be an exhaustive or real-world application; rather, it's meant for you to have fun with and explore the libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314dde04",
   "metadata": {},
   "source": [
    "### First steps\n",
    "\n",
    "Let us start with the basics. For this I will start with a simulated Logit and then give a classification example on the famous Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccc9b30",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5416147",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4477dce4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.5'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# Keras works with 3 different backend as of Keras 3.0: jax, tensorflow, pytorch\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41e26b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 861us/step\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 613us/step - accuracy: 0.5613 - loss: 0.6859\n",
      "Loss: 0.6880548000335693\n",
      "Accuracy: 0.5450000166893005\n",
      "Learned Weights: [[-0.02999177]\n",
      " [ 0.06566735]\n",
      " [ 0.12366135]\n",
      " [-0.02722669]\n",
      " [ 0.20459336]\n",
      " [-0.10692663]\n",
      " [-0.01676666]\n",
      " [-0.10032698]\n",
      " [ 0.00637742]\n",
      " [-0.13347614]\n",
      " [ 0.01018549]\n",
      " [-0.0195969 ]\n",
      " [-0.10994846]]\n",
      "Learned Bias: [0.14966391]\n"
     ]
    }
   ],
   "source": [
    "# Logit\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Generate some sample data in numpy, the perfered data structure of Keras.\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(1000, 13) # X is of dimension (1000,13) with values between 0-2\n",
    "y = (np.random.rand(1000, 1)>0.5).astype(int) # y of dimension (1000,1) value 0 or 1\n",
    "\n",
    "# Create a Keras sequential model\n",
    "def create_model(input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a single Dense layer with input dim 13 = number of features and output a single neuron \n",
    "    # Sigmoid activation function \n",
    "    model.add(Dense(units=1, input_dim=input_dim, activation='sigmoid'))\n",
    "\n",
    "    # Parameter of optimization\n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = create_model(13)\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Display the loss and accuracy\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the learned parameters\n",
    "weights, bias = model.layers[0].get_weights()\n",
    "\n",
    "\n",
    "print(\"Learned Weights:\", weights)\n",
    "print(\"Learned Bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1a2df658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\ap\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9842 - loss: 0.1315 \n",
      "Loss: 0.17954681813716888\n",
      "Accuracy: 0.9800000190734863\n",
      "Learned Weights: [[ 1.4497296   1.1178024  -0.949229  ]\n",
      " [ 0.81996244 -0.7238664  -1.6423622 ]\n",
      " [-2.6269233  -0.59547603  2.0923035 ]\n",
      " [-1.1622708  -0.07232351  2.0126088 ]]\n",
      "Learned Bias: [ 0.29411888  0.34348464 -0.6376028 ]\n"
     ]
    }
   ],
   "source": [
    "# The same but with iris\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df = pd.read_csv(\"G:/Github/Advanced_programming/data/Chap5/iris.csv\")\n",
    "X = df.iloc[:,0:4].values\n",
    "y = df.iloc[:,4:5].values\n",
    "\n",
    "# This time the target value is categorical. We need to one-hot encode it.\n",
    "enc = OneHotEncoder()\n",
    "y_encoded = enc.fit_transform(y).toarray()\n",
    "\n",
    "# Create a Keras sequential model\n",
    "def create_model(input_dim, output_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a single Dense layer with input dim 13 = number of features and output a single neuron \n",
    "    # Sigmoid activation function \n",
    "    model.add(Dense(units=output_dim, input_dim=input_dim, activation='sigmoid'))\n",
    "\n",
    "    # Parameter of optimization\n",
    "    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = create_model(X.shape[1],y_encoded.shape[1])\n",
    "model.fit(X, y_encoded, epochs=1000, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Display the loss and accuracy\n",
    "loss, accuracy = model.evaluate(X, y_encoded)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the learned parameters\n",
    "weights, bias = model.layers[0].get_weights()\n",
    "\n",
    "\n",
    "print(\"Learned Weights:\", weights)\n",
    "print(\"Learned Bias:\", bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787f77be",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcf2cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f59083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6926108598709106\n",
      "Accuracy: 0.5199999809265137\n",
      "Learned Weights: [[ 0.02706285  0.05788565  0.0088839   0.03678624 -0.06637889 -0.10481282\n",
      "  -0.07036275  0.1973196   0.04217785 -0.08689839  0.02863655 -0.08786355\n",
      "   0.03931884]]\n",
      "Learned Bias: [-0.00564162]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate some sample data directly using PyTorch\n",
    "torch.manual_seed(0)\n",
    "X = 2 * torch.rand((1000, 13))  # X is of dimension (1000, 13) with values between 0-2\n",
    "y = (torch.rand((1000, 1)) > 0.5).float()  # y of dimension (1000, 1) with values 0 or 1\n",
    "\n",
    "# Define the logistic regression model in PyTorch\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(13, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = LogisticRegressionModel()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions on the training set\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "\n",
    "# Display the loss and accuracy\n",
    "loss = criterion(predictions, y)\n",
    "accuracy = ((predictions > 0.5).float() == y).float().mean().item()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the learned parameters\n",
    "learned_weights = model.linear.weight.data.numpy()\n",
    "learned_bias = model.linear.bias.data.numpy()\n",
    "\n",
    "print(\"Learned Weights:\", learned_weights)\n",
    "print(\"Learned Bias:\", learned_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6bb4d896",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\ap\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3437958359718323\n",
      "Accuracy: 0.9599999785423279\n",
      "Learned Weights: [[ 0.584271    1.2367085  -1.4465144  -0.16792874]\n",
      " [ 0.33822206  0.00474036  0.28807935 -0.558156  ]\n",
      " [-0.29904243 -0.11848312  0.8245503   0.9773022 ]]\n",
      "Learned Bias: [-0.3053165   0.18019998 -0.509855  ]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the Iris dataset\n",
    "df = pd.read_csv(\"G:/Github/Advanced_programming/data/Chap5/iris.csv\")\n",
    "X = df.iloc[:, 0:4].values\n",
    "y = df.iloc[:, 4:5].values\n",
    "\n",
    "# Use OneHotEncoder for one-hot encoding\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.FloatTensor(X)\n",
    "y_tensor = torch.LongTensor(y_encoded)  # Use LongTensor for class indices\n",
    "\n",
    "# Define the multi-class classification model in PyTorch\n",
    "class SoftmaxRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SoftmaxRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = SoftmaxRegression(X.shape[1], len(label_encoder.classes_))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_tensor)\n",
    "    loss = criterion(outputs, y_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions on the training set\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_tensor).argmax(dim=1)\n",
    "\n",
    "# Display the loss and accuracy\n",
    "accuracy = (predictions == y_tensor).float().mean().item()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the learned parameters\n",
    "learned_weights = model.linear.weight.data.numpy()\n",
    "learned_bias = model.linear.bias.data.numpy()\n",
    "\n",
    "print(\"Learned Weights:\", learned_weights)\n",
    "print(\"Learned Bias:\", learned_bias)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f2a7f6",
   "metadata": {},
   "source": [
    "### Basic ANN\n",
    "\n",
    "In the next examples we will only work with simulated data. The extension from a Logit to a Deep Neural Network is straightforward: add layers. If you want to read more about the maths behind you can go to https://github.com/Kwirtz/Masters-thesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed08215",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "136f8408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\ap\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:85: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 788us/step - accuracy: 0.5796 - loss: 0.6748\n",
      "Loss: 0.6750162839889526\n",
      "Accuracy: 0.5740000009536743\n",
      "Learned Weights: [[-0.4196801   0.00680731  0.1597391  -0.3605326   0.78662187 -0.7330399 ]\n",
      " [-0.25520197  0.30476987  0.15540582  0.3816981  -0.28491476 -0.19996245]\n",
      " [-0.3662898  -0.00177413  0.39288893 -0.37944707  0.5912493   0.17201051]\n",
      " [ 0.49356464 -0.02677529  0.124106   -0.1027741   0.63024676 -0.38500684]\n",
      " [-0.06351601  0.17742766  0.71370953  0.51008147 -0.08762832  0.18800147]\n",
      " [ 0.08288312  0.16383585 -0.1735739  -0.04432764 -0.52658564 -0.41826892]\n",
      " [ 0.3629843   0.19836167  0.10548487 -0.43004385  0.4467683  -0.33218703]\n",
      " [-0.00850612  0.28580937 -0.07770222  0.13697319 -0.4945716  -0.5668438 ]\n",
      " [-0.37907127 -0.14924575  0.12196314  0.28334576 -0.2120233  -0.33470097]\n",
      " [ 0.24744192 -0.88164234 -0.25995627 -0.46744072 -0.13421354  0.3023705 ]\n",
      " [-0.38520855  0.20971037  0.3745023  -0.06832974  0.25311705  0.4214803 ]\n",
      " [-0.10953771 -0.7632984   0.01853607 -0.20044635 -0.29556426  0.535035  ]\n",
      " [ 0.3295459  -0.4149053  -0.3336336  -0.03990697 -0.09239416  0.02365619]]\n",
      "Learned Bias: [-0.10495035 -0.12418561  0.04260169  0.03271822  0.00048212 -0.07787642]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Generate some sample data in numpy, the perfered data structure of Keras.\n",
    "np.random.seed(0)\n",
    "X = 2 * np.random.rand(1000, 13) # X is of dimension (1000,13) with values between 0-2\n",
    "y = (np.random.rand(1000, 1)>0.5).astype(int) # y of dimension (1000,1) value 0 or 1\n",
    "\n",
    "# Create a Keras sequential model\n",
    "def create_model(input_dim):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=6, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(units=3, input_dim=6, activation='tanh'))\n",
    "    model.add(Dense(units=1, input_dim=3, activation='sigmoid'))\n",
    "\n",
    "    # Parameter of optimization\n",
    "    model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "model = create_model(13)\n",
    "model.fit(X, y, epochs=1000, verbose=0)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Display the loss and accuracy\n",
    "loss, accuracy = model.evaluate(X, y)\n",
    "print(\"Loss:\", loss)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the learned parameters\n",
    "weights, bias = model.layers[0].get_weights()\n",
    "\n",
    "\n",
    "print(\"Learned Weights:\", weights)\n",
    "print(\"Learned Bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acb555e",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecaadb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6912749409675598\n",
      "Accuracy: 0.5429999828338623\n",
      "Learned Weights (Hidden Layer): [[ 0.27676916  0.22004108 -0.01971278 -0.17247663 -0.15132871 -0.08123114\n",
      "  -0.22645132  0.27516913  0.00834347 -0.26574063 -0.13962209 -0.0897949\n",
      "  -0.22472286]\n",
      " [-0.05971444 -0.17539938 -0.07008361 -0.2085401  -0.00351137 -0.18862922\n",
      "   0.11213145  0.06330274 -0.02105505  0.16538756  0.13970141 -0.01688642\n",
      "   0.00141582]\n",
      " [ 0.10929148 -0.23835024  0.05312441  0.02026546 -0.00936524 -0.07474421\n",
      "  -0.13400179  0.08516776 -0.19301999  0.10432585 -0.22973296  0.07563709\n",
      "   0.02641906]\n",
      " [-0.14505142 -0.13391112 -0.08370666 -0.13641164 -0.08411308 -0.07115243\n",
      "   0.2067233   0.00343365  0.22648242 -0.06908101 -0.22084124 -0.07287527\n",
      "   0.13551839]\n",
      " [-0.20449051  0.22203276 -0.01413966  0.23885901 -0.05770554 -0.23124187\n",
      "  -0.149548    0.04953567 -0.1916856   0.25017902  0.25512478 -0.06451254\n",
      "   0.2108624 ]\n",
      " [ 0.08420499 -0.02861391 -0.27390605 -0.2299333  -0.21122538 -0.02463877\n",
      "   0.2118786   0.07191713 -0.04574926  0.168909    0.11965138 -0.1949521\n",
      "   0.06842463]]\n",
      "Learned Bias (Hidden Layer): [ 0.018651    0.11211627  0.0266352  -0.09343319 -0.11675032  0.2162541 ]\n",
      "Learned Weights (Output Layer): [[-0.5046418   0.02660025  0.20601118]]\n",
      "Learned Bias (Output Layer): [-0.12636623]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Generate some sample data directly using PyTorch\n",
    "torch.manual_seed(0)\n",
    "X = 2 * torch.rand((1000, 13))  # X is of dimension (1000, 13) with values between 0-2\n",
    "y = (torch.rand((1000, 1)) > 0.5).float()  # y of dimension (1000, 1) with values 0 or 1\n",
    "\n",
    "# Define the Artificial Neural Network model in PyTorch\n",
    "class NeuralNetworkModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(NeuralNetworkModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, 6)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(6, 3)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.layer3 = nn.Linear(3, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.tanh(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model, loss function, and optimizer\n",
    "model = NeuralNetworkModel(input_dim=13)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Make predictions on the training set\n",
    "with torch.no_grad():\n",
    "    predictions = model(X)\n",
    "\n",
    "# Display the loss and accuracy\n",
    "loss = criterion(predictions, y)\n",
    "accuracy = ((predictions > 0.5).float() == y).float().mean().item()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the learned parameters\n",
    "learned_weights_hidden = model.layer1.weight.data.numpy()\n",
    "learned_bias_hidden = model.layer1.bias.data.numpy()\n",
    "learned_weights_output = model.layer3.weight.data.numpy()\n",
    "learned_bias_output = model.layer3.bias.data.numpy()\n",
    "\n",
    "print(\"Learned Weights (Hidden Layer):\", learned_weights_hidden)\n",
    "print(\"Learned Bias (Hidden Layer):\", learned_bias_hidden)\n",
    "print(\"Learned Weights (Output Layer):\", learned_weights_output)\n",
    "print(\"Learned Bias (Output Layer):\", learned_bias_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c1f60",
   "metadata": {},
   "source": [
    "### Basic CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d982a95",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34df7930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\ap\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 30ms/step - accuracy: 0.5303 - loss: 0.8553\n",
      "Epoch 2/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5086 - loss: 0.6942\n",
      "Epoch 3/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5359 - loss: 0.6927\n",
      "Epoch 4/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.5719 - loss: 0.6922\n",
      "Epoch 5/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5009 - loss: 0.6927\n",
      "Epoch 6/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.6064 - loss: 0.6903\n",
      "Epoch 7/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.5424 - loss: 0.6884\n",
      "Epoch 8/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5221 - loss: 0.6916\n",
      "Epoch 9/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5323 - loss: 0.6941\n",
      "Epoch 10/10\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.5523 - loss: 0.6917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x14a208c2210>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Generate random images for training\n",
    "num_samples = 1000\n",
    "image_size = (64, 64, 3)  # Assuming RGB images\n",
    "\n",
    "# Randomly generate images\n",
    "X_train = np.random.rand(num_samples, *image_size)\n",
    "y_train = np.random.randint(2, size=num_samples)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer with 32 filters, kernel size 3x3, and ReLU activation\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=image_size))\n",
    "# Max pooling layer with pool size 2x2\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# Flatten layer to transition from convolutional layers to fully connected layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully connected layer with 128 neurons and ReLU activation\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Output layer with 1 neuron and sigmoid activation for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cff81c",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09a75b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Generate random images for training\n",
    "num_samples = 1000\n",
    "image_size = (3, 64, 64)  # Assuming RGB images\n",
    "\n",
    "# Randomly generate images\n",
    "X_train = torch.rand(num_samples, *image_size)\n",
    "y_train = torch.randint(2, size=(num_samples,))\n",
    "\n",
    "# Define the CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.float().view(-1, 1))\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# You can now use the trained model to make predictions on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba8e5bb",
   "metadata": {},
   "source": [
    "### Basic RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c252ce9c",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea4c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import SimpleRNN, Dense\n",
    "\n",
    "# Generate some sample data directly using NumPy\n",
    "np.random.seed(0)\n",
    "seq_length = 10\n",
    "num_samples = 1000\n",
    "\n",
    "X_train = np.random.randn(num_samples, seq_length, 1)\n",
    "y_train = np.sum(X_train, axis=1)\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a SimpleRNN layer with 32 units and tanh activation\n",
    "model.add(SimpleRNN(32, activation='tanh', input_shape=(seq_length, 1)))\n",
    "\n",
    "# Add a Dense layer with one unit for regression\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb9c552",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07c916a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNNModel(\n",
      "  (rnn): RNN(1, 32, batch_first=True)\n",
      "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Generate some sample data directly using NumPy\n",
    "np.random.seed(0)\n",
    "seq_length = 10\n",
    "num_samples = 1000\n",
    "\n",
    "X_train = np.random.randn(num_samples, seq_length, 1).astype(np.float32)\n",
    "y_train = np.sum(X_train, axis=1).astype(np.float32)\n",
    "\n",
    "# Convert NumPy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "\n",
    "# Define the RNN model\n",
    "class SimpleRNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleRNNModel, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
    "        return out\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 1\n",
    "hidden_size = 32\n",
    "output_size = 1\n",
    "model = SimpleRNNModel(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Print the model summary\n",
    "print(model)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# You can now use the trained model to make predictions on new data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e87d81",
   "metadata": {},
   "source": [
    "### Basic GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a2f4884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Using cached torch_geometric-2.5.0-py3-none-any.whl.metadata (64 kB)\n",
      "Collecting tqdm (from torch_geometric)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: scipy in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from torch_geometric) (1.12.0)\n",
      "Requirement already satisfied: fsspec in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from torch_geometric) (2024.2.0)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from torch_geometric) (3.1.3)\n",
      "Collecting aiohttp (from torch_geometric)\n",
      "  Using cached aiohttp-3.9.3-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: requests in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from torch_geometric) (2.31.0)\n",
      "Collecting pyparsing (from torch_geometric)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from torch_geometric) (1.4.1.post1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from torch_geometric) (5.9.8)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->torch_geometric)\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->torch_geometric)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->torch_geometric)\n",
      "  Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->torch_geometric)\n",
      "  Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->torch_geometric)\n",
      "  Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl.metadata (32 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from requests->torch_geometric) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from requests->torch_geometric) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from requests->torch_geometric) (2024.2.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from scikit-learn->torch_geometric) (3.3.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda3\\envs\\ap\\lib\\site-packages (from tqdm->torch_geometric) (0.4.6)\n",
      "Using cached torch_geometric-2.5.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached aiohttp-3.9.3-cp312-cp312-win_amd64.whl (363 kB)\n",
      "Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 0.0/103.2 kB ? eta -:--:--\n",
      "   ----------- --------------------------- 30.7/103.2 kB 660.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  102.4/103.2 kB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  102.4/103.2 kB 1.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- 103.2/103.2 kB 664.3 kB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached frozenlist-1.4.1-cp312-cp312-win_amd64.whl (50 kB)\n",
      "Using cached multidict-6.0.5-cp312-cp312-win_amd64.whl (27 kB)\n",
      "Using cached yarl-1.9.4-cp312-cp312-win_amd64.whl (76 kB)\n",
      "Installing collected packages: tqdm, pyparsing, multidict, frozenlist, attrs, yarl, aiosignal, aiohttp, torch_geometric\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 attrs-23.2.0 frozenlist-1.4.1 multidict-6.0.5 pyparsing-3.1.2 torch_geometric-2.5.0 tqdm-4.66.2 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Ignored the following versions that require a different python version: 0.10.0 Requires-Python >=3.6.0, <3.8.0; 0.11.0 Requires-Python >=3.6.0, <3.8.0; 0.11.1 Requires-Python >=3.6.0, <3.8.0; 0.4.0 Requires-Python >=3.6,<3.7; 0.4.0b0 Requires-Python >=3.6,<3.7; 0.4.1 Requires-Python >=3.5.0, <3.7.0; 0.5.0 Requires-Python >=3.5.0, <3.7.0; 0.6.0 Requires-Python >=3.5.0, <3.7.0; 0.6.1 Requires-Python >=3.5.0, <3.7.0; 0.7.0 Requires-Python >=3.5.0, <3.8.0; 0.7.1 Requires-Python >=3.5.0, <3.8.0; 0.7.2 Requires-Python >=3.5.0, <3.8.0; 0.7.3 Requires-Python >=3.5.0, <3.8.0; 0.8.0 Requires-Python >=3.5.0, <3.8.0; 0.8.1 Requires-Python >=3.5.0, <3.8.0; 0.8.2 Requires-Python >=3.5.0, <3.8.0; 0.8.3 Requires-Python >=3.5.0, <3.8.0; 0.8.4 Requires-Python >=3.5.0, <3.8.0; 0.9.0 Requires-Python >=3.6.0, <3.8.0; 1.0.0 Requires-Python >=3.6.0, <3.8.0; 1.0.0rc1 Requires-Python >=3.6.0, <3.8.0; 1.1.0 Requires-Python >=3.6.0, <3.9.0; 1.2.0 Requires-Python >=3.6.0, <3.9.0; 1.2.1 Requires-Python >=3.6.0, <3.9.0\n",
      "ERROR: Could not find a version that satisfies the requirement stellargraph (from versions: none)\n",
      "ERROR: No matching distribution found for stellargraph\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install stellargraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86ae27",
   "metadata": {},
   "source": [
    "#### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3d5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.layer import GraphConvolution\n",
    "from keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Generate the Karate Club graph\n",
    "graph = nx.karate_club_graph()\n",
    "\n",
    "# Node features (randomly initialized for illustration purposes)\n",
    "feature_size = 16\n",
    "node_features = np.random.randn(len(graph.nodes), feature_size)\n",
    "\n",
    "# Labels (1 for \"Mr. Hi\" club, 0 for \"John A\" club)\n",
    "labels = np.array([1 if graph.nodes[node][\"club\"] == \"Mr. Hi\" else 0 for node in graph.nodes])\n",
    "\n",
    "# Create a StellarGraph object\n",
    "G = StellarGraph(features=node_features, edges=graph.edges())\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_nodes, test_nodes = train_test_split(G.nodes(), test_size=0.2, random_state=42)\n",
    "\n",
    "train_targets = labels[train_nodes]\n",
    "test_targets = labels[test_nodes]\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "target_encoder = LabelEncoder()\n",
    "train_targets = target_encoder.fit_transform(train_targets)\n",
    "test_targets = target_encoder.transform(test_targets)\n",
    "\n",
    "# Define the graph convolutional network (GCN) model using Keras\n",
    "gcn = GraphConvolution(layer_sizes=[hidden_dim, output_dim], activations=[\"relu\", \"softmax\"], generator=G)\n",
    "x_input, x_output = gcn.in_out_tensors()\n",
    "\n",
    "# Create the Keras model\n",
    "model = Model(inputs=x_input, outputs=x_output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "epochs = 100\n",
    "history = model.fit(\n",
    "    generator.flow(train_nodes, train_targets),\n",
    "    epochs=epochs,\n",
    "    validation_data=generator.flow(test_nodes, test_targets),\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_metrics = model.evaluate(generator.flow(test_nodes, test_targets))\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ee42f6",
   "metadata": {},
   "source": [
    "#### Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c94744fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'karate_club_graph' from 'torch_geometric.utils' (d:\\anaconda3\\envs\\ap\\Lib\\site-packages\\torch_geometric\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Data, DataLoader\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GraphConv\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m karate_club_graph\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m binary_cross_entropy_with_logits\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Generate the Karate Club graph\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'karate_club_graph' from 'torch_geometric.utils' (d:\\anaconda3\\envs\\ap\\Lib\\site-packages\\torch_geometric\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.utils import karate_club_graph\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "\n",
    "# Generate the Karate Club graph\n",
    "edge_index, _ = karate_club_graph()\n",
    "\n",
    "# Node features (randomly initialized for illustration purposes)\n",
    "num_nodes = edge_index.max().item() + 1\n",
    "x = torch.randn(num_nodes, 16)\n",
    "\n",
    "# Labels (1 for \"Mr. Hi\" club, 0 for \"John A\" club)\n",
    "y = torch.tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# Create a PyTorch Geometric Data object\n",
    "data = Data(x=x, edge_index=edge_index, y=y)\n",
    "\n",
    "# Define the Graph Neural Network model\n",
    "class GNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GNNModel, self).__init__()\n",
    "        self.conv1 = GraphConv(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = GraphConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_dim = 16\n",
    "hidden_dim = 32\n",
    "output_dim = 1\n",
    "model = GNNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = binary_cross_entropy_with_logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(data)\n",
    "    loss = criterion(outputs.view(-1), data.y.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# You can now use the trained model to make predictions on new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8af8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.karate_club_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ceb490ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph named \"Zachary's Karate Club\" with 34 nodes and 78 edges\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c794ff54",
   "metadata": {},
   "source": [
    "\n",
    "<a name=\"PowerBI\"></a>\n",
    "## Power BI\n",
    "\n",
    "Power BI, developed by Microsoft, is a business analytics tool that enables users to visualize and share insights from their data. It provides a suite of tools for data analysis, interactive reporting, and sharing of insights across an organization or embedding them in an app or website. Here are some key components and features of Power BI:\n",
    "\n",
    "1. **Data Connectivity:** Power BI can connect to a wide variety of data sources, both on-premises and in the cloud. It supports connections to databases, spreadsheets, online services, and more. Popular data sources include Microsoft Excel, SQL Server, Azure services, SharePoint, and many third-party applications.\n",
    "\n",
    "2. **Data Transformation and Modeling:** Power BI allows users to transform and shape their data using a graphical interface. This includes cleaning, filtering, and transforming data to make it suitable for analysis. Power BI also supports data modeling, allowing users to create relationships between different data tables.\n",
    "\n",
    "3. **Visualization:** One of the key strengths of Power BI is its ability to create compelling visualizations. Users can build interactive reports and dashboards using a drag-and-drop interface. The tool provides a wide range of visualization options, including charts, graphs, maps, and tables.\n",
    "\n",
    "4. **Data Analysis Expressions (DAX):** Power BI uses a formula language called Data Analysis Expressions (DAX) for creating custom calculations and aggregations. DAX is similar to Excel formulas and allows users to define complex calculations for their data.\n",
    "\n",
    "5. **Power Query:** Power BI includes a data connectivity and preparation tool called Power Query. It allows users to connect, import, and transform data from various sources before loading it into Power BI.\n",
    "\n",
    "6. **Power BI Service:** The Power BI Service is a cloud-based platform for sharing, collaborating, and publishing Power BI reports and dashboards. Users can publish their reports to the Power BI Service, making them accessible to others within or outside their organization.\n",
    "\n",
    "7. **Power BI Desktop:** Power BI Desktop is a free, standalone application that allows users to create reports and dashboards on their local machines before publishing them to the Power BI Service.\n",
    "\n",
    "8. **Power BI Mobile:** Power BI offers mobile apps for iOS and Android devices, allowing users to access and interact with their reports and dashboards on the go.\n",
    "\n",
    "Overall, Power BI is a powerful tool for turning raw data into meaningful insights, making it a popular choice for businesses and analysts looking to visualize and analyze their data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"PreviousProject\"></a>\n",
    "## Previous Project\n",
    "\n",
    "Each year student tend to cluster around a topic (Scrapping, Dashboards, Apps, Computer vision, ...)\n",
    "\n",
    "- Open cv + tensorflow to detect face using webcam and control pc\n",
    "- News - buzzword - map\n",
    "- Sentiment analysis on glassdoor\n",
    "- Discord bot to connect on spotify and linked with youtube\n",
    "- F1 dashboard\n",
    "- mtse = research advanced\n",
    "- Detect country of plate\n",
    "- Finance bot following trending and selling with different strategy\n",
    "- Table tennis\n",
    "- Bot hearthstone (expert system)\n",
    "- Se loger.com\n",
    "- jvc + Linkedin\n",
    "- Yuka like\n",
    "- Recipe proposal\n",
    "- Grocery shopping list preparation and price comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
