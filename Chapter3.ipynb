{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b808d71",
   "metadata": {
    "id": "80f714d7"
   },
   "source": [
    "# Chapter 3: GPU programming.\n",
    "\n",
    "This chapter and the next will make extensive use of GPUs. Sadly, depending on your machine, it can be impossible to use it in python. For example, at the time I do this tutorial, Radeon is not supported. Also some of your laptop may not have GPUs. These reasons pushed me to run the next Chapter on google collab (https://colab.research.google.com/). If you are still interested in using your own GPUs here are some advice/links that might help you:\n",
    "\n",
    "- https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781\n",
    "- https://www.youtube.com/watch?v=hHWkvEcDBO0\n",
    "- https://www.youtube.com/watch?v=KZFn0dvPZUQ\n",
    "- https://towardsdatascience.com/installing-tensorflow-gpu-in-ubuntu-20-04-4ee3ca4cb75d\n",
    "- https://medium.com/analytics-vidhya/install-tensorflow-2-for-amd-gpus-87e8d7aeb812\n",
    "\n",
    "Why do we want to use GPUs ?\n",
    "\n",
    "GPUs hardware is designed for data parallelism. Maximum throughput is achieved when you are computing the same operations on many different elements at once.\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\n",
    "\n",
    "Among the tasks that do significantly benefit from parallel processing is deep learning. Some tasks can't be done in parallel (When you need to have the same object in memory, e.g calculating a series like fibonnaci).\n",
    "\n",
    "One thing that could be nice would be to write the same code as normal (numpy, pandas,..) but just to run computation on a GPU. This would make it easier to parallelize processes. Some companies/university/people are working on this kind of libraries and that's what we are going to use in this section.\n",
    "\n",
    "Structure:\n",
    "- [Collab](#Collab)\n",
    "- [CuPy](#CuPy)\n",
    "- [CuDF and CuML](#CuDF)\n",
    "- [Numba](#Numba)\n",
    "- [TODO](#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c08a575",
   "metadata": {
    "id": "Wx4t6km9Y-HG"
   },
   "source": [
    "<a name=\"Collab\"></a>\n",
    "## Google Collab\n",
    "\n",
    "Stockage is limited to 60 gb (see on the left) \n",
    "\n",
    "Ram is limited to 12 gb (top right)\n",
    "\n",
    "You can select gpu accelerated from modify->parameter of the notebook. \n",
    "\n",
    "Create text block and code block\n",
    "\n",
    "You can create section.\n",
    "\n",
    "Resembles jupyter notebook and uses ipynb.\n",
    "\n",
    "Change color and shortcut in utils\n",
    "\n",
    "The os you are connected to is ubuntu\n",
    "To run something in the terminal you need to add \"!\" in front of it\n",
    "\n",
    "Python already installed.\n",
    "\n",
    "Session are limited in time.\n",
    "\n",
    "To use GPU go to Execution and modify the type of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf8034b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AJmlvE_ZGNN",
    "outputId": "def23907-8a82-482c-e8ec-e9d1bf4910c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\n"
     ]
    }
   ],
   "source": [
    "# Check Python Version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f64ea",
   "metadata": {
    "id": "LIssTuYUZGUc"
   },
   "outputs": [],
   "source": [
    "# Check Ubuntu Version\n",
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cda8787",
   "metadata": {
    "id": "tIiWoMMUZGdF"
   },
   "outputs": [],
   "source": [
    "# Check CUDA/cuDNN Version\n",
    "!nvcc -V && which nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2aa124",
   "metadata": {
    "id": "ScwHa6_tZGjT"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b913a",
   "metadata": {
    "id": "t3PMptfnZM6I"
   },
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de5dbc",
   "metadata": {
    "id": "06dbf475"
   },
   "source": [
    "<a name=\"CuPy\"></a>\n",
    "## CuPy\n",
    "\n",
    "CuPy is the GPU equivalent to Numpy. CuPy uses the same methods that numpy so cost entry going from Numpy to CuPy is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae817d02",
   "metadata": {
    "id": "09eb2ba0"
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ef583d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdda01d7",
    "outputId": "fd15f378-1e91-4ad6-933e-811ed645e8fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = cp.arange(6).reshape(2, 3).astype('f')\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1405d78",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "592b4150",
    "outputId": "29b8588c-3896-4ede-bdf7-9b821c5f45c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 2.5, 3.5], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bf1c0e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81685878",
    "outputId": "af074922-5ff2-41c1-9ed9-5b1935991d68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3., 12.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff555e02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55989285",
    "outputId": "acc4c858-d579-45c6-d842-b089a642d930"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 14],\n",
       "       [14, 50]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.dot(z.T).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36bce47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f8aa1fe",
    "outputId": "68c340af-c2e4-4036-b9e1-7d18745431cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 2, 3, 4],\n",
      "       [5, 6, 7, 8, 9]])\n",
      "int64\n",
      "(2, 5)\n",
      "(40, 8)\n",
      "<CUDA Device 0>\n"
     ]
    }
   ],
   "source": [
    "ary = cp.arange(10).reshape((2,5))\n",
    "print(ary.dtype)\n",
    "print(ary.shape)\n",
    "print(ary.strides)\n",
    "print(ary.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4612e5fb-356a-4cc1-afbc-1e4becbc2906",
   "metadata": {},
   "source": [
    "You can easily convert numpy array to cupy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b027c689",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64a6f833",
    "outputId": "b6e6de06-4fa3-450e-fe88-c5de128af856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu: [0 1 2 3 4 5 6 7 8 9]\n",
      "gpu: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "ary_cpu = np.arange(10)\n",
    "ary_gpu = cp.asarray(ary_cpu)\n",
    "print('cpu:', ary_cpu)\n",
    "print('gpu:', ary_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57ccd7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0538afb",
    "outputId": "74cadfda-5588-4c66-a0e9-27b0b6fb84e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "ary_cpu_returned = cp.asnumpy(ary_gpu)\n",
    "print(repr(ary_cpu_returned))\n",
    "print(type(ary_cpu_returned))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519bad1-8e3a-4caf-b118-8adc43cccde1",
   "metadata": {},
   "source": [
    "Ufunc are also available on cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c339156a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88146714",
    "outputId": "6422fe1e-8990-4aff-daa6-9ef0dd3c220d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  8 10 12 14 16 18]\n",
      "[1.00000000e+00 6.06530660e-01 1.35335283e-01 1.11089965e-02\n",
      " 3.35462628e-04 3.72665317e-06 1.52299797e-08 2.28973485e-11\n",
      " 1.26641655e-14 2.57675711e-18]\n",
      "16.881943016134134\n",
      "[4.33906114 4.67377457 1.08905314 9.21676987 6.70516547 3.11660347\n",
      " 3.57749795 2.01394103 6.62182583 5.56588289]\n"
     ]
    }
   ],
   "source": [
    "print(ary_gpu * 2)\n",
    "print(cp.exp(-0.5 * ary_gpu**2))\n",
    "print(cp.linalg.norm(ary_gpu))\n",
    "print(cp.random.normal(loc=5, scale=2.0, size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5231863f",
   "metadata": {
    "id": "21999581"
   },
   "source": [
    "You may notice a slight pause when you run these functions the first time. This is because CuPy has to compile the CUDA functions on the fly, and then cache them to disk for reuse in the future. Let's compare some performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb47f263",
   "metadata": {
    "id": "84b926e4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cupy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc706154",
   "metadata": {
    "id": "EGXYAoCsuRzH"
   },
   "source": [
    "Let's compare a simple multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0721c11b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwWZZgWltL2d",
    "outputId": "42a1bf3e-6d53-40d0-add0-be83471ac436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 1.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "# small example taken from here https://giters.com/cupy/cupy/issues/4891?amp=1\n",
    "\n",
    "a_cpu = np.ones((1000, 20000), dtype='float32')\n",
    "b_cpu = np.ones((20000, 2000), dtype='float32')\n",
    "z_cpu = np.matmul(a_cpu, b_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638d34f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjMNpUW6tVGL",
    "outputId": "94b71fbd-fd05-45c4-9f61-bf4e3b371660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 3125.66 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 5: 221 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "# small example taken from here https://giters.com/cupy/cupy/issues/4891?amp=1\n",
    "\n",
    "a_gpu = cp.ones((1000, 20000), dtype='float32')\n",
    "b_gpu = cp.ones((20000, 2000), dtype='float32')\n",
    "z_gpu = cp.matmul(a_gpu, b_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2108e71",
   "metadata": {
    "id": "qXF6TLvluX5F"
   },
   "source": [
    "Now the analytical solution of OLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e4cee5",
   "metadata": {
    "id": "StBLvSPPuOUt"
   },
   "outputs": [],
   "source": [
    "X_cpu = np.random.rand(20000, 1000).astype('f')\n",
    "Y_cpu = np.random.rand(20000, 1).astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91325f73",
   "metadata": {
    "id": "GNDBFSIcuOfw"
   },
   "outputs": [],
   "source": [
    "X_gpu = cp.asarray(X_cpu,dtype='float32')\n",
    "Y_gpu = cp.asarray(Y_cpu,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60432f2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xevtJL_XvGZC",
    "outputId": "e96bdd20-2329-4c38-c9d9-b50046f556ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 483 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "beta = np.matmul(np.linalg.inv(np.matmul(X_cpu.T,X_cpu)),np.matmul(X_cpu.T,Y_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81647e4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLaruNuovPdg",
    "outputId": "cc10489d-df31-4377-f4f0-772bc5fe893f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 5: 55.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "beta = np.matmul(np.linalg.inv(np.matmul(X_gpu.T,X_gpu)),np.matmul(X_gpu.T,Y_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fafe9f",
   "metadata": {
    "id": "SSPMczq3vcw6"
   },
   "source": [
    "This does not mean that GPUs are always faster. When are they worst ? \n",
    "Read more here https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8046ff16",
   "metadata": {
    "id": "5118068f"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_data/california_housing_train.csv\")\n",
    "intercept = np.ones(len(df))\n",
    "\n",
    "y = np.array(df[\"median_house_value\"],dtype='float32')\n",
    "X = np.array(df.drop([\"median_house_value\"],axis=1),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216c7fc",
   "metadata": {
    "id": "1fe97c84"
   },
   "outputs": [],
   "source": [
    "y_gpu = cp.asarray(y,dtype='float32')\n",
    "X_gpu = cp.asarray(X,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b27bb5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e47e471b",
    "outputId": "f035fb6c-884b-4d5b-f344-9865ecc8d4f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 5: 412 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "beta = np.matmul(np.linalg.inv(np.matmul(X.T,X)),np.matmul(X.T,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf007f2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e88673b",
    "outputId": "1d3f9cdf-6489-4269-bb93-5798e04e718c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.37 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 5: 761 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "beta = cp.matmul(cp.linalg.inv(cp.matmul(X_gpu.T,X_gpu)),cp.matmul(X_gpu.T,y_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd188b9c",
   "metadata": {
    "id": "MZEXmUHuwK-d"
   },
   "source": [
    "Also cupy works best when using ufunc but like we have seen in the introduction, not every operation can be done using ufunc. To overcome this issue you can create your own \"Kernel\". (read more here https://docs.cupy.dev/en/stable/user_guide/kernel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d54e6b",
   "metadata": {
    "id": "0323daec"
   },
   "outputs": [],
   "source": [
    "# Example taken from docs\n",
    "\n",
    "x = cp.arange(10, dtype=np.float32).reshape(2, 5)\n",
    "y = cp.arange(5, dtype=np.float32)\n",
    "\n",
    "add_reverse = cp.ElementwiseKernel(\n",
    "    'T x, raw T y', \n",
    "    'T z',\n",
    "    '''\n",
    "    z = x + y[_ind.size() - i - 1];\n",
    "    ''',\n",
    "    'add_reverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf5c072",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6be33d5a",
    "outputId": "7399bdfe-2ae2-4ac9-b487-40ae0e2f6a2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1311454e-01, -1.0437331e+27,  1.8231753e+00, -1.1218646e+17,\n",
       "         4.2748837e+00],\n",
       "       [ 9.0000000e+00,  9.0000000e+00,  9.0000000e+00,  9.0000000e+00,\n",
       "         9.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_reverse(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf9153-c19d-4cd4-a010-764a49254fb2",
   "metadata": {
    "id": "RxtbxS_pxwHc"
   },
   "source": [
    "You can find even more complex custom CUDA kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b710dd61-63dd-435c-a4c1-e5d765b9b1ed",
   "metadata": {},
   "source": [
    "<a name=\"CuDF\"></a>\n",
    "## CuDF and CuML\n",
    "\n",
    "CuDF is develeopped by rapidsai (https://rapids.ai/) and like CuPY the goal is to have the features of pandas using GPUs. CuML is also develeopped by rapidsai (https://rapids.ai/) and this time the library we want to apply GPUs is Scikit-learn. Installing them on google collab is a bit complex so we will directly use their google collab cells:https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&sandboxMode=true&scrollTo=JI7UTXbhaBon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fd872",
   "metadata": {
    "id": "17a6e786"
   },
   "source": [
    "<a name=\"Numba\"></a>\n",
    "## Numba\n",
    "\n",
    "Numba is a just-in-time (https://en.wikipedia.org/wiki/Just-in-time_compilation), type-specializing, function compiler for accelerating numerically-focused Python. That's a long list, so let's break down those terms: \n",
    "\n",
    "- function compiler: Numba compiles Python functions, not entire applications, and not parts of functions. Numba does not replace your Python interpreter, but is just another Python module that can turn a function into a (usually) faster function.\n",
    "- type-specializing: Numba speeds up your function by generating a specialized implementation for the specific data types you are using. Python functions are designed to operate on generic data types, which makes them very flexible, but also very slow. In practice, you only will call a function with a small number of argument types, so Numba will generate a fast implementation for each set of types.\n",
    "- just-in-time: Numba translates functions when they are first called. This ensures the compiler knows what argument types you will be using. This also allows Numba to be used interactively in a Jupyter notebook just as easily as a traditional application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1819171",
   "metadata": {
    "id": "34e20a27"
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import math\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    # Implementation from https://en.wikipedia.org/wiki/Hypot\n",
    "    x = abs(x);\n",
    "    y = abs(y);\n",
    "    t = min(x, y);\n",
    "    x = max(x, y);\n",
    "    t = t / x;\n",
    "    return x * math.sqrt(1+t*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58132e26",
   "metadata": {
    "id": "1f08cabd"
   },
   "source": [
    "The first time we call hypot, the compiler is triggered and compiles a machine code implementation for float inputs. Numba also saves the original Python implementation of the function in the .py_func attribute, so we can call the original Python code to make sure we get the same answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c39650",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12ff089e",
    "outputId": "31d0d1a8-dc7b-46cb-8af7-bef644d55f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 729428.58 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 5: 327 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hypot(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c453fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d2e33d8",
    "outputId": "1119b6b9-266b-41f4-8a79-42ba3fca3374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 15.48 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 5: 946 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hypot.py_func(10,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605c5a55",
   "metadata": {
    "id": "c2de54b0"
   },
   "source": [
    "How does numba works ? From https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1\n",
    "\n",
    "![numba](img/numba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf7f25",
   "metadata": {
    "id": "3Dy8-fx5FKkv"
   },
   "source": [
    "Numba can also be used to create new numpy universal function.\n",
    "https://numba.pydata.org/numba-doc/dev/user/vectorize.html\n",
    "\n",
    "Numba’s vectorize allows Python functions taking scalar input arguments to be used as NumPy ufuncs. Creating a traditional NumPy ufunc is not the most straightforward process and involves writing some C code. Numba makes this easy. Using the vectorize() decorator, Numba can compile a pure Python function into a ufunc that operates over NumPy arrays as fast as traditional ufuncs written in C.\n",
    "\n",
    "Using vectorize(), you write your function as operating over input scalars, rather than arrays. Numba will generate the surrounding loop (or kernel) allowing efficient iteration over the actual inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cee3a3d3",
   "metadata": {
    "id": "EYgO08umFOwi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.81 s ± 27 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "\n",
    "from numba import vectorize, float64\n",
    "import numpy as np\n",
    "\n",
    "@vectorize([float64(float64, float64)])\n",
    "def f(x, y):\n",
    "    x - y\n",
    "    return x + y\n",
    "N = 100000000\n",
    "A = np.array(np.random.sample(N), dtype=np.float64)\n",
    "B = np.array(np.random.sample(N), dtype=np.float64)\n",
    "result = f(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "626d6a6d",
   "metadata": {
    "id": "3-5PWWZ1FSPA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.84 s ± 21.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "from numba import vectorize, float64\n",
    "import numpy as np\n",
    "\n",
    "@vectorize([float64(float64, float64)], target = \"parallel\")\n",
    "def f(x, y):\n",
    "    x - y\n",
    "    return x + y\n",
    "\n",
    "N = 100000000\n",
    "A = np.array(np.random.sample(N), dtype=np.float64)\n",
    "B = np.array(np.random.sample(N), dtype=np.float64)\n",
    "f(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2df1c9a7-445b-406d-bb9b-f304965a6a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.96 s ± 4.09 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def f(x, y):\n",
    "    x - y\n",
    "    return x + y\n",
    "\n",
    "N = 100000000\n",
    "A = np.array(np.random.sample(N), dtype=np.float64)\n",
    "B = np.array(np.random.sample(N), dtype=np.float64)\n",
    "f(A,B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99ff0c5",
   "metadata": {
    "id": "gSho8Fn5GF4-"
   },
   "source": [
    "Numba also supports GPU programming.\n",
    "\n",
    "Read more here:\n",
    "- https://numba.pydata.org/numba-doc/0.42.0/cuda/kernels.html\n",
    "- https://stackoverflow.com/questions/4391162/cuda-determining-threads-per-block-blocks-per-grid\n",
    "- https://en.wikipedia.org/wiki/CUDA\n",
    "- https://nyu-cds.github.io/python-numba/05-cuda/\n",
    "- https://numba.pydata.org/numba-doc/latest/cuda/ufunc.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983c0a19",
   "metadata": {
    "id": "615z3WnZGIw9"
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def642e8",
   "metadata": {
    "id": "DMrZ4A8WGMEm"
   },
   "outputs": [],
   "source": [
    "# list of devices\n",
    "print(cuda.gpus)\n",
    "# Select your device\n",
    "numba.cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13fc08-727b-4e77-a3e9-e4a0d810fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['int64(int64, int64)'], target='cuda')\n",
    "def add_ufunc(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f2b196a-22d8-4c62-b6c2-5a7b4f1315cb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_ufunc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-c4ddd7e76cec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a+b:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_ufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'b_col + c:\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_ufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'add_ufunc' is not defined"
     ]
    }
   ],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([10, 20, 30, 40])\n",
    "b_col = b[:, np.newaxis] # b as column array\n",
    "c = np.arange(4*4).reshape((4,4))\n",
    "\n",
    "print('a+b:\\n', add_ufunc(a, b))\n",
    "print('\\n\\n')\n",
    "print('b_col + c:\\n', add_ufunc(b_col, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b8664-cd40-4880-a18d-eb842dde0a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.add(b_col, c)   # NumPy on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec616ca-8884-402e-8a08-b62c4de80a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_ufunc(b_col, c) # Numba on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec62c8",
   "metadata": {
    "id": "sp4q4h74GqFw"
   },
   "source": [
    "Why is the GPU slower ?\n",
    "* Our inputs are too small: the GPU achieves performance through parallelism, operating on thousands of values at once. Our test inputs have only 4 and 16 integers, respectively. We need a much larger array to even keep the GPU busy.\n",
    "* Our calculation is too simple: Sending a calculation to the GPU involves quite a bit of overhead compared to calling a function on the CPU. If our calculation does not involve enough math operations (often called \"arithmetic intensity\"), then the GPU will spend most of its time waiting for data to move around.\n",
    "* We copy the data to and from the GPU: While including the copy time can be realistic for a single function, often we want to run several GPU operations in sequence. In those cases, it makes sense to send data to the GPU and keep it there until all of our processing is complete.\n",
    "* Our data types are larger than necessary: Our example uses int64 when we probably don't need it. Scalar code using data types that are 32 and 64-bit run basically the same speed on the CPU, but 64-bit data types have a significant performance cost on the GPU. Basic arithmetic on 64-bit floats can be anywhere from 2x (Pascal-architecture Tesla) to 24x (Maxwell-architecture GeForce) slower than 32-bit floats. NumPy defaults to 64-bit data types when creating arrays, so it is important to set the dtype attribute or use the ndarray.astype() method to pick 32-bit types when you need them.\n",
    "\n",
    "Let's see a bigger example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eec456",
   "metadata": {
    "id": "DceSUrtnGtVO"
   },
   "outputs": [],
   "source": [
    "import math  # Note that for the CUDA target, we need to use the scalar functions from the math module, not NumPy\n",
    "\n",
    "SQRT_2PI = np.float32((2*math.pi)**0.5)  # Precompute this constant as a float32.  Numba will inline it at compile time.\n",
    "\n",
    "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
    "def gaussian_pdf(x, mean, sigma):\n",
    "    '''Compute the value of a Gaussian probability density function at x with given mean and sigma.'''\n",
    "    return math.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * SQRT_2PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e206a",
   "metadata": {
    "id": "UAi9tsYHGvFs"
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(-3, 3, size=1000000).astype(np.float32)\n",
    "mean = np.float32(0.0)\n",
    "sigma = np.float32(1.0)\n",
    "\n",
    "# Quick test\n",
    "gaussian_pdf(x[0], 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2026410",
   "metadata": {
    "id": "VsNLu8WVGw-E"
   },
   "outputs": [],
   "source": [
    "import scipy.stats # for definition of gaussian distribution\n",
    "norm_pdf = scipy.stats.norm\n",
    "%timeit norm_pdf.pdf(x, loc=mean, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee5b1cc",
   "metadata": {
    "id": "sTIBljD5Gyyw"
   },
   "outputs": [],
   "source": [
    "%timeit gaussian_pdf(x, mean, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f9ca4-5057-4554-b033-7a077f51993b",
   "metadata": {},
   "source": [
    "Of course not everything can be vectorized so you'll need to create your own cuda kernel. Here are two example of the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c403b2f-ae64-42b9-a105-ff89058d7315",
   "metadata": {
    "id": "txwMZUtCGX02"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] *= 2 # do the computation\n",
    "\n",
    "# Host code   \n",
    "data = np.ones(256)\n",
    "threadsperblock = 256\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8409b6b6-e63d-47e2-b44a-d52fb9f181b8",
   "metadata": {
    "id": "PoabnjYxGaUu"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.full((24, 12), 3, np.float) # matrix containing all 3's\n",
    "B = np.full((12, 22), 4, np.float) # matrix containing all 4's\n",
    "\n",
    "# Copy the arrays to the device\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device for the result\n",
    "C_global_mem = cuda.device_array((24, 22))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host\n",
    "C = C_global_mem.copy_to_host()\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea3c80c",
   "metadata": {
    "id": "IB1T3oPUFadr"
   },
   "source": [
    "## Numba limitations\n",
    "\n",
    "Numba accelerates your code. So why should'nt we use it for everything if it's has simple as putting a decorator in front of a function ?\n",
    "\n",
    "Well it's not that simple.\n",
    "\n",
    "Numba is numerically-focused: Currently, Numba is focused on numerical data types, like int, float, and complex. There is very limited string processing support, and many string use cases are not going to work well on the GPU. To get best results with Numba, you will likely be using NumPy arrays. When you run a function that uses string or dict, python ignores the jit decorator and run the function as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c499c7",
   "metadata": {
    "id": "V2jU98yTFc39"
   },
   "outputs": [],
   "source": [
    "@jit()\n",
    "def cannot_compile(x):\n",
    "    return x['key']\n",
    "\n",
    "cannot_compile(dict(key='hey heres your value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3750e33b",
   "metadata": {
    "id": "JOlFtwHVFjCe"
   },
   "source": [
    "To avoid this type of behavior (we want an error message and not just a warning) we add the argument nopython = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514da7c",
   "metadata": {
    "id": "JDeZXx1wFdXa"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def cannot_compile(x):\n",
    "    return x['key']\n",
    "\n",
    "cannot_compile(dict(key='hey heres your value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765437b",
   "metadata": {
    "id": "0888630d"
   },
   "source": [
    "<a name=\"TODO\"></a>\n",
    "## TODO\n",
    "\n",
    "code review: \n",
    "- https://www.programcreek.com/python/example/111769/cupy.ElementwiseKernel"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
