{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80f714d7",
   "metadata": {
    "id": "80f714d7"
   },
   "source": [
    "# Chapter 3: GPU programming.\n",
    "\n",
    "This chapter and the next will make extensive use of GPUs. Sadly, depending on your machine, it can be impossible to use it in python. For example, at the time I do this tutorial, Radeon is not supported. Also some of your laptop may not have GPUs. These reasons pushed me to run the next Chapter on google collab (https://colab.research.google.com/). If you are still interested in using your own GPUs here are some advice/links that might help you:\n",
    "\n",
    "- https://towardsdatascience.com/installing-tensorflow-with-cuda-cudnn-and-gpu-support-on-windows-10-60693e46e781\n",
    "- https://www.youtube.com/watch?v=hHWkvEcDBO0\n",
    "- https://www.youtube.com/watch?v=KZFn0dvPZUQ\n",
    "- https://towardsdatascience.com/installing-tensorflow-gpu-in-ubuntu-20-04-4ee3ca4cb75d\n",
    "- https://medium.com/analytics-vidhya/install-tensorflow-2-for-amd-gpus-87e8d7aeb812\n",
    "\n",
    "Why do we want to use GPUs ?\n",
    "\n",
    "GPUs hardware is designed for data parallelism. Maximum throughput is achieved when you are computing the same operations on many different elements at once.\n",
    "\n",
    "https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html\n",
    "\n",
    "Among the tasks that do significantly benefit from parallel processing is deep learning. Some tasks can't be used in parallel (When you need to have the same object in memory, e.g calculating a series like fibonnaci).\n",
    "\n",
    "One thing that could be nice would be to write the same code as normal (numpy, pandas,..) but just to run computation on a GPU. This would make it easier to parallelize processes. Some companies/university/people are working on this kind of libraries and that's what we are going to use in this section.\n",
    "\n",
    "Structure:\n",
    "- [Collab](#Collab)\n",
    "- [CuPy](#CuPy)\n",
    "- [Numba](#Numba)\n",
    "- [CuDF and CuML](#CuDF)\n",
    "- [TODO](#TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wx4t6km9Y-HG",
   "metadata": {
    "id": "Wx4t6km9Y-HG"
   },
   "source": [
    "<a name=\"Collab\"></a>\n",
    "## Google Collab\n",
    "\n",
    "Stockage is limited to 60 gb (see on the left) \n",
    "\n",
    "Ram is limited to 12 gb (top right)\n",
    "\n",
    "You can select gpu accelerated from modify->parameter of the notebook. \n",
    "\n",
    "Create text block and code block\n",
    "\n",
    "You can create section.\n",
    "\n",
    "Resembles jupyter notebook and uses ipynb.\n",
    "\n",
    "Change color and shortcut in utils\n",
    "\n",
    "The os you are connected to is ubuntu\n",
    "To run something in the terminal you need to add \"!\" in front of it\n",
    "\n",
    "Python already installed.\n",
    "\n",
    "Session are limited in time.\n",
    "\n",
    "To use GPU go to Execution and modify the type of execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7AJmlvE_ZGNN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AJmlvE_ZGNN",
    "outputId": "def23907-8a82-482c-e8ec-e9d1bf4910c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.11\n"
     ]
    }
   ],
   "source": [
    "# Check Python Version\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LIssTuYUZGUc",
   "metadata": {
    "id": "LIssTuYUZGUc"
   },
   "outputs": [],
   "source": [
    "# Check Ubuntu Version\n",
    "!lsb_release -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tIiWoMMUZGdF",
   "metadata": {
    "id": "tIiWoMMUZGdF"
   },
   "outputs": [],
   "source": [
    "# Check CUDA/cuDNN Version\n",
    "!nvcc -V && which nvcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ScwHa6_tZGjT",
   "metadata": {
    "id": "ScwHa6_tZGjT"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t3PMptfnZM6I",
   "metadata": {
    "id": "t3PMptfnZM6I"
   },
   "outputs": [],
   "source": [
    "!cat /proc/cpuinfo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dbf475",
   "metadata": {
    "id": "06dbf475"
   },
   "source": [
    "<a name=\"CuPy\"></a>\n",
    "## CuPy\n",
    "\n",
    "CuPy is the GPU equivalent to Numpy. CuPy uses the same methods that numpy so cost entry going from Numpy to CuPy is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eb2ba0",
   "metadata": {
    "id": "09eb2ba0"
   },
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdda01d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdda01d7",
    "outputId": "fd15f378-1e91-4ad6-933e-811ed645e8fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 2.],\n",
       "       [3., 4., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = cp.arange(6).reshape(2, 3).astype('f')\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b4150",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "592b4150",
    "outputId": "29b8588c-3896-4ede-bdf7-9b821c5f45c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, 2.5, 3.5], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81685878",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81685878",
    "outputId": "af074922-5ff2-41c1-9ed9-5b1935991d68"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3., 12.], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55989285",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "55989285",
    "outputId": "acc4c858-d579-45c6-d842-b089a642d930"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5, 14],\n",
       "       [14, 50]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.dot(z.T).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8aa1fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f8aa1fe",
    "outputId": "68c340af-c2e4-4036-b9e1-7d18745431cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[0, 1, 2, 3, 4],\n",
      "       [5, 6, 7, 8, 9]])\n",
      "int64\n",
      "(2, 5)\n",
      "(40, 8)\n",
      "<CUDA Device 0>\n"
     ]
    }
   ],
   "source": [
    "ary = cp.arange(10).reshape((2,5))\n",
    "print(repr(ary))\n",
    "print(ary.dtype)\n",
    "print(ary.shape)\n",
    "print(ary.strides)\n",
    "print(ary.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6f833",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "64a6f833",
    "outputId": "b6e6de06-4fa3-450e-fe88-c5de128af856"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu: [0 1 2 3 4 5 6 7 8 9]\n",
      "gpu: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "ary_cpu = np.arange(10)\n",
    "ary_gpu = cp.asarray(ary_cpu)\n",
    "print('cpu:', ary_cpu)\n",
    "print('gpu:', ary_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0538afb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b0538afb",
    "outputId": "74cadfda-5588-4c66-a0e9-27b0b6fb84e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "ary_cpu_returned = cp.asnumpy(ary_gpu)\n",
    "print(repr(ary_cpu_returned))\n",
    "print(type(ary_cpu_returned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88146714",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88146714",
    "outputId": "6422fe1e-8990-4aff-daa6-9ef0dd3c220d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  2  4  6  8 10 12 14 16 18]\n",
      "[1.00000000e+00 6.06530660e-01 1.35335283e-01 1.11089965e-02\n",
      " 3.35462628e-04 3.72665317e-06 1.52299797e-08 2.28973485e-11\n",
      " 1.26641655e-14 2.57675711e-18]\n",
      "16.881943016134134\n",
      "[4.33906114 4.67377457 1.08905314 9.21676987 6.70516547 3.11660347\n",
      " 3.57749795 2.01394103 6.62182583 5.56588289]\n"
     ]
    }
   ],
   "source": [
    "print(ary_gpu * 2)\n",
    "print(cp.exp(-0.5 * ary_gpu**2))\n",
    "print(cp.linalg.norm(ary_gpu))\n",
    "print(cp.random.normal(loc=5, scale=2.0, size=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21999581",
   "metadata": {
    "id": "21999581"
   },
   "source": [
    "You may notice a slight pause when you run these functions the first time. This is because CuPy has to compile the CUDA functions on the fly, and then cache them to disk for reuse in the future. Let's compare some performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b926e4",
   "metadata": {
    "id": "84b926e4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cupy as cp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EGXYAoCsuRzH",
   "metadata": {
    "id": "EGXYAoCsuRzH"
   },
   "source": [
    "Let's compare a simple multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GwWZZgWltL2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwWZZgWltL2d",
    "outputId": "42a1bf3e-6d53-40d0-add0-be83471ac436"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 1.2 s per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "# small example taken from here https://giters.com/cupy/cupy/issues/4891?amp=1\n",
    "\n",
    "a_cpu = np.ones((1000, 20000), dtype='float32')\n",
    "b_cpu = np.ones((20000, 2000), dtype='float32')\n",
    "z_cpu = np.matmul(a_cpu, b_cpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LjMNpUW6tVGL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjMNpUW6tVGL",
    "outputId": "94b71fbd-fd05-45c4-9f61-bf4e3b371660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 3125.66 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 5: 221 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "# small example taken from here https://giters.com/cupy/cupy/issues/4891?amp=1\n",
    "\n",
    "a_gpu = cp.ones((1000, 20000), dtype='float32')\n",
    "b_gpu = cp.ones((20000, 2000), dtype='float32')\n",
    "z_gpu = cp.matmul(a_gpu, b_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qXF6TLvluX5F",
   "metadata": {
    "id": "qXF6TLvluX5F"
   },
   "source": [
    "Now the analytical solution of OLS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "StBLvSPPuOUt",
   "metadata": {
    "id": "StBLvSPPuOUt"
   },
   "outputs": [],
   "source": [
    "X_cpu = np.random.rand(20000, 1000).astype('f')\n",
    "Y_cpu = np.random.rand(20000, 1).astype('f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GNDBFSIcuOfw",
   "metadata": {
    "id": "GNDBFSIcuOfw"
   },
   "outputs": [],
   "source": [
    "X_gpu = cp.asarray(X_cpu,dtype='float32')\n",
    "Y_gpu = cp.asarray(Y_cpu,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xevtJL_XvGZC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xevtJL_XvGZC",
    "outputId": "e96bdd20-2329-4c38-c9d9-b50046f556ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 483 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "beta = np.matmul(np.linalg.inv(np.matmul(X_cpu.T,X_cpu)),np.matmul(X_cpu.T,Y_cpu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vLaruNuovPdg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vLaruNuovPdg",
    "outputId": "cc10489d-df31-4377-f4f0-772bc5fe893f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 5: 55.6 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "beta = np.matmul(np.linalg.inv(np.matmul(X_gpu.T,X_gpu)),np.matmul(X_gpu.T,Y_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SSPMczq3vcw6",
   "metadata": {
    "id": "SSPMczq3vcw6"
   },
   "source": [
    "This does not mean that GPUs are always faster. When are they worst ? \n",
    "Read more here https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5118068f",
   "metadata": {
    "id": "5118068f"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"sample_data/california_housing_train.csv\")\n",
    "intercept = np.ones(len(df))\n",
    "\n",
    "y = np.array(df[\"median_house_value\"],dtype='float32')\n",
    "X = np.array(df.drop([\"median_house_value\"],axis=1),dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe97c84",
   "metadata": {
    "id": "1fe97c84"
   },
   "outputs": [],
   "source": [
    "y_gpu = cp.asarray(y,dtype='float32')\n",
    "X_gpu = cp.asarray(X,dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47e471b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e47e471b",
    "outputId": "f035fb6c-884b-4d5b-f344-9865ecc8d4f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 loops, best of 5: 412 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "beta = np.matmul(np.linalg.inv(np.matmul(X.T,X)),np.matmul(X.T,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88673b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6e88673b",
    "outputId": "1d3f9cdf-6489-4269-bb93-5798e04e718c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 6.37 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000 loops, best of 5: 761 µs per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "beta = cp.matmul(cp.linalg.inv(cp.matmul(X_gpu.T,X_gpu)),cp.matmul(X_gpu.T,y_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MZEXmUHuwK-d",
   "metadata": {
    "id": "MZEXmUHuwK-d"
   },
   "source": [
    "Also cupy works best when using ufunc but like we have seen in tthe introduction, not every operation can be done using ufunc. To overcome this issue you can create your own \"Kernel\". This Kernel is written in C++ so don't expect to understand everything directly. (read more here https://docs.cupy.dev/en/stable/user_guide/kernel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0323daec",
   "metadata": {
    "id": "0323daec"
   },
   "outputs": [],
   "source": [
    "# Example taken from docs\n",
    "\n",
    "x = cp.arange(10, dtype=np.float32).reshape(2, 5)\n",
    "y = cp.arange(5, dtype=np.float32)\n",
    "\n",
    "add_reverse = cp.ElementwiseKernel(\n",
    "    'T x, raw T y', \n",
    "    'T z',\n",
    "    '''\n",
    "    z = x + y[_ind.size() - i - 1];\n",
    "    ''',\n",
    "    'add_reverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be33d5a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6be33d5a",
    "outputId": "7399bdfe-2ae2-4ac9-b487-40ae0e2f6a2b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.1311454e-01, -1.0437331e+27,  1.8231753e+00, -1.1218646e+17,\n",
       "         4.2748837e+00],\n",
       "       [ 9.0000000e+00,  9.0000000e+00,  9.0000000e+00,  9.0000000e+00,\n",
       "         9.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_reverse(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RxtbxS_pxwHc",
   "metadata": {
    "id": "RxtbxS_pxwHc"
   },
   "source": [
    "You can find even more complex custom CUDA kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a6e786",
   "metadata": {
    "id": "17a6e786"
   },
   "source": [
    "<a name=\"Numba\"></a>\n",
    "## Numba\n",
    "\n",
    "Numba is a just-in-time (https://en.wikipedia.org/wiki/Just-in-time_compilation), type-specializing, function compiler for accelerating numerically-focused Python. That's a long list, so let's break down those terms: \n",
    "\n",
    "- function compiler: Numba compiles Python functions, not entire applications, and not parts of functions. Numba does not replace your Python interpreter, but is just another Python module that can turn a function into a (usually) faster function.\n",
    "- type-specializing: Numba speeds up your function by generating a specialized implementation for the specific data types you are using. Python functions are designed to operate on generic data types, which makes them very flexible, but also very slow. In practice, you only will call a function with a small number of argument types, so Numba will generate a fast implementation for each set of types.\n",
    "- just-in-time: Numba translates functions when they are first called. This ensures the compiler knows what argument types you will be using. This also allows Numba to be used interactively in a Jupyter notebook just as easily as a traditional application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e20a27",
   "metadata": {
    "id": "34e20a27"
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import math\n",
    "\n",
    "@jit\n",
    "def hypot(x, y):\n",
    "    # Implementation from https://en.wikipedia.org/wiki/Hypot\n",
    "    x = abs(x);\n",
    "    y = abs(y);\n",
    "    t = min(x, y);\n",
    "    x = max(x, y);\n",
    "    t = t / x;\n",
    "    return x * math.sqrt(1+t*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f08cabd",
   "metadata": {
    "id": "1f08cabd"
   },
   "source": [
    "The first time we call hypot, the compiler is triggered and compiles a machine code implementation for float inputs. Numba also saves the original Python implementation of the function in the .py_func attribute, so we can call the original Python code to make sure we get the same answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ff089e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12ff089e",
    "outputId": "31d0d1a8-dc7b-46cb-8af7-bef644d55f71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 729428.58 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 5: 327 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hypot(10,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2e33d8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d2e33d8",
    "outputId": "1119b6b9-266b-41f4-8a79-42ba3fca3374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 15.48 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1000000 loops, best of 5: 946 ns per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit hypot.py_func(10,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2de54b0",
   "metadata": {
    "id": "c2de54b0"
   },
   "source": [
    "How does numba works ? From https://towardsdatascience.com/speed-up-your-algorithms-part-2-numba-293e554c5cc1\n",
    "\n",
    "![numba](img/numba.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3Dy8-fx5FKkv",
   "metadata": {
    "id": "3Dy8-fx5FKkv"
   },
   "source": [
    "Numba can also be used to create new numpy universal function.\n",
    "https://numba.pydata.org/numba-doc/dev/user/vectorize.html\n",
    "\n",
    "Numba’s vectorize allows Python functions taking scalar input arguments to be used as NumPy ufuncs. Creating a traditional NumPy ufunc is not the most straightforward process and involves writing some C code. Numba makes this easy. Using the vectorize() decorator, Numba can compile a pure Python function into a ufunc that operates over NumPy arrays as fast as traditional ufuncs written in C.\n",
    "\n",
    "Using vectorize(), you write your function as operating over input scalars, rather than arrays. Numba will generate the surrounding loop (or kernel) allowing efficient iteration over the actual inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EYgO08umFOwi",
   "metadata": {
    "id": "EYgO08umFOwi"
   },
   "outputs": [],
   "source": [
    "%%timeit \n",
    "\n",
    "from numba import vectorize, float64\n",
    "\n",
    "@vectorize([float64(float64, float64)])\n",
    "def f(x, y):\n",
    "    return x + y\n",
    "N = 10000\n",
    "A = np.array(np.random.sample(N), dtype=dtype)\n",
    "B = np.array(np.random.sample(N), dtype=dtype)\n",
    "f(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3-5PWWZ1FSPA",
   "metadata": {
    "id": "3-5PWWZ1FSPA"
   },
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "from numba import vectorize, float64\n",
    "\n",
    "@vectorize([float64(float64, float64)], target = \"parallel\")\n",
    "def f(x, y):\n",
    "    return x + y\n",
    "\n",
    "N = 10000\n",
    "A = np.array(np.random.sample(N), dtype=dtype)\n",
    "B = np.array(np.random.sample(N), dtype=dtype)\n",
    "f(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dR23UHHkFUPG",
   "metadata": {
    "id": "dR23UHHkFUPG"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['float32(float32, float32, float32)',\n",
    "            'float64(float64, float64, float64)'])\n",
    "def cu_discriminant(a, b, c):\n",
    "    return math.sqrt(b ** 2 - 4 * a * c)\n",
    "\n",
    "N = 10000\n",
    "dtype = np.float32\n",
    "\n",
    "# prepare the input\n",
    "A = np.array(np.random.sample(N), dtype=dtype)\n",
    "B = np.array(np.random.sample(N) + 10, dtype=dtype)\n",
    "C = np.array(np.random.sample(N), dtype=dtype)\n",
    "\n",
    "D = cu_discriminant(A, B, C)\n",
    "\n",
    "print(D)  # print result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gSho8Fn5GF4-",
   "metadata": {
    "id": "gSho8Fn5GF4-"
   },
   "source": [
    "Numba also supports GPU programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615z3WnZGIw9",
   "metadata": {
    "id": "615z3WnZGIw9"
   },
   "outputs": [],
   "source": [
    "import numba\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DMrZ4A8WGMEm",
   "metadata": {
    "id": "DMrZ4A8WGMEm"
   },
   "outputs": [],
   "source": [
    "# list of devices\n",
    "print(cuda.gpus)\n",
    "# Select your device\n",
    "numba.cuda.select_device(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1R6rbOXaGRHy",
   "metadata": {
    "id": "1R6rbOXaGRHy"
   },
   "source": [
    "CUDA has an execution model unlike the traditional sequential model used for programming CPUs. In CUDA, the code you write will be executed by multiple threads at once (often hundreds or thousands). Your solution will be modeled by defining a thread hierarchy of grid, blocks, and threads.\n",
    "\n",
    "Numba also exposes three kinds of GPU memory:\n",
    "\n",
    "* global device memory\n",
    "* shared memory\n",
    "* local memory\n",
    "\n",
    "For all but the simplest algorithms, it is important that you carefully consider how to use and access memory in order to minimize bandwidth requirements and contention.\n",
    "\n",
    "NVIDIA recommends that programmers focus on following those recommendations to achieve the best performance:\n",
    "\n",
    "* Find ways to parallelize sequential code\n",
    "* Minimize data transfers between the host and the device\n",
    "* Adjust kernel launch configuration to maximize device utilization\n",
    "* Ensure global memory accesses are coalesced\n",
    "* Minimize redundant accesses to global memory whenever possible\n",
    "* Avoid different execution paths within the same warp\n",
    "\n",
    "https://stackoverflow.com/questions/4391162/cuda-determining-threads-per-block-blocks-per-grid\n",
    "https://en.wikipedia.org/wiki/CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "txwMZUtCGX02",
   "metadata": {
    "id": "txwMZUtCGX02"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# CUDA kernel\n",
    "@cuda.jit\n",
    "def my_kernel(io_array):\n",
    "    pos = cuda.grid(1)\n",
    "    if pos < io_array.size:\n",
    "        io_array[pos] *= 2 # do the computation\n",
    "\n",
    "# Host code   \n",
    "data = np.ones(256)\n",
    "threadsperblock = 256\n",
    "blockspergrid = math.ceil(data.shape[0] / threadsperblock)\n",
    "my_kernel[blockspergrid, threadsperblock](data)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PoabnjYxGaUu",
   "metadata": {
    "id": "PoabnjYxGaUu"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform matrix multiplication of C = A * B\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "    if row < C.shape[0] and col < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[row, k] * B[k, col]\n",
    "        C[row, col] = tmp\n",
    "        \n",
    "# Host code\n",
    "\n",
    "# Initialize the data arrays\n",
    "A = np.full((24, 12), 3, np.float) # matrix containing all 3's\n",
    "B = np.full((12, 22), 4, np.float) # matrix containing all 4's\n",
    "\n",
    "# Copy the arrays to the device\n",
    "A_global_mem = cuda.to_device(A)\n",
    "B_global_mem = cuda.to_device(B)\n",
    "\n",
    "# Allocate memory on the device for the result\n",
    "C_global_mem = cuda.device_array((24, 22))\n",
    "\n",
    "# Configure the blocks\n",
    "threadsperblock = (16, 16)\n",
    "blockspergrid_x = int(math.ceil(A.shape[0] / threadsperblock[0]))\n",
    "blockspergrid_y = int(math.ceil(B.shape[1] / threadsperblock[1]))\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "# Start the kernel \n",
    "matmul[blockspergrid, threadsperblock](A_global_mem, B_global_mem, C_global_mem)\n",
    "\n",
    "# Copy the result back to the host\n",
    "C = C_global_mem.copy_to_host()\n",
    "\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vp6uEZpYGeaw",
   "metadata": {
    "id": "Vp6uEZpYGeaw"
   },
   "outputs": [],
   "source": [
    "from numba import vectorize\n",
    "import numpy as np\n",
    "\n",
    "@vectorize(['int64(int64, int64)'], target='cuda')\n",
    "def add_ufunc(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o2K0V_IXGgl2",
   "metadata": {
    "id": "o2K0V_IXGgl2"
   },
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([10, 20, 30, 40])\n",
    "b_col = b[:, np.newaxis] # b as column array\n",
    "c = np.arange(4*4).reshape((4,4))\n",
    "\n",
    "print('a+b:\\n', add_ufunc(a, b))\n",
    "print()\n",
    "print('b_col + c:\\n', add_ufunc(b_col, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DiJUnfVLGhAo",
   "metadata": {
    "id": "DiJUnfVLGhAo"
   },
   "source": [
    "A lot of things just happened! Numba automatically:\n",
    "\n",
    "* Compiled a CUDA kernel to execute the ufunc operation in parallel over all the input elements.\n",
    "* Allocated GPU memory for the inputs and the output.\n",
    "* Copied the input data to the GPU.\n",
    "* Executed the CUDA kernel with the correct kernel dimensions given the input sizes.\n",
    "* Copied the result back from the GPU to the CPU.\n",
    "* Returned the result as a NumPy array on the host.\n",
    "\n",
    "This is very convenient for testing, but copying data back and forth between the CPU and GPU can be slow and hurt performance. In the next tutorial notebook, you'll learn about device management, memory allocation, and using CuPy arrays with Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9wS1puF1Gmp0",
   "metadata": {
    "id": "9wS1puF1Gmp0"
   },
   "outputs": [],
   "source": [
    "%timeit np.add(b_col, c)   # NumPy on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nQym4ficGn8K",
   "metadata": {
    "id": "nQym4ficGn8K"
   },
   "outputs": [],
   "source": [
    "%timeit add_ufunc(b_col, c) # Numba on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sp4q4h74GqFw",
   "metadata": {
    "id": "sp4q4h74GqFw"
   },
   "source": [
    "Why is the GPU slower ?\n",
    "* Our inputs are too small: the GPU achieves performance through parallelism, operating on thousands of values at once. Our test inputs have only 4 and 16 integers, respectively. We need a much larger array to even keep the GPU busy.\n",
    "* Our calculation is too simple: Sending a calculation to the GPU involves quite a bit of overhead compared to calling a function on the CPU. If our calculation does not involve enough math operations (often called \"arithmetic intensity\"), then the GPU will spend most of its time waiting for data to move around.\n",
    "* We copy the data to and from the GPU: While including the copy time can be realistic for a single function, often we want to run several GPU operations in sequence. In those cases, it makes sense to send data to the GPU and keep it there until all of our processing is complete.\n",
    "* Our data types are larger than necessary: Our example uses int64 when we probably don't need it. Scalar code using data types that are 32 and 64-bit run basically the same speed on the CPU, but 64-bit data types have a significant performance cost on the GPU. Basic arithmetic on 64-bit floats can be anywhere from 2x (Pascal-architecture Tesla) to 24x (Maxwell-architecture GeForce) slower than 32-bit floats. NumPy defaults to 64-bit data types when creating arrays, so it is important to set the dtype attribute or use the ndarray.astype() method to pick 32-bit types when you need them.\n",
    "\n",
    "Let's see a bigger example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DceSUrtnGtVO",
   "metadata": {
    "id": "DceSUrtnGtVO"
   },
   "outputs": [],
   "source": [
    "import math  # Note that for the CUDA target, we need to use the scalar functions from the math module, not NumPy\n",
    "\n",
    "SQRT_2PI = np.float32((2*math.pi)**0.5)  # Precompute this constant as a float32.  Numba will inline it at compile time.\n",
    "\n",
    "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
    "def gaussian_pdf(x, mean, sigma):\n",
    "    '''Compute the value of a Gaussian probability density function at x with given mean and sigma.'''\n",
    "    return math.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * SQRT_2PI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UAi9tsYHGvFs",
   "metadata": {
    "id": "UAi9tsYHGvFs"
   },
   "outputs": [],
   "source": [
    "x = np.random.uniform(-3, 3, size=1000000).astype(np.float32)\n",
    "mean = np.float32(0.0)\n",
    "sigma = np.float32(1.0)\n",
    "\n",
    "# Quick test\n",
    "gaussian_pdf(x[0], 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VsNLu8WVGw-E",
   "metadata": {
    "id": "VsNLu8WVGw-E"
   },
   "outputs": [],
   "source": [
    "import scipy.stats # for definition of gaussian distribution\n",
    "norm_pdf = scipy.stats.norm\n",
    "%timeit norm_pdf.pdf(x, loc=mean, scale=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sTIBljD5Gyyw",
   "metadata": {
    "id": "sTIBljD5Gyyw"
   },
   "outputs": [],
   "source": [
    "%timeit gaussian_pdf(x, mean, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IB1T3oPUFadr",
   "metadata": {
    "id": "IB1T3oPUFadr"
   },
   "source": [
    "## Numba limitations\n",
    "\n",
    "Numba accelerates your code. So why should'nt we use it for everything if it's has simple as putting a decorator in front of a function ?\n",
    "\n",
    "Well it's not that simple.\n",
    "\n",
    "Numba is numerically-focused: Currently, Numba is focused on numerical data types, like int, float, and complex. There is very limited string processing support, and many string use cases are not going to work well on the GPU. To get best results with Numba, you will likely be using NumPy arrays. When you run a function that uses string or dict, python ignores the jit decorator and run the function as normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "V2jU98yTFc39",
   "metadata": {
    "id": "V2jU98yTFc39"
   },
   "outputs": [],
   "source": [
    "@jit()\n",
    "def cannot_compile(x):\n",
    "    return x['key']\n",
    "\n",
    "cannot_compile(dict(key='hey heres your value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JOlFtwHVFjCe",
   "metadata": {
    "id": "JOlFtwHVFjCe"
   },
   "source": [
    "To avoid this type of behavior (we want an error message and not just a warning) we add the argument nopython = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JDeZXx1wFdXa",
   "metadata": {
    "id": "JDeZXx1wFdXa"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def cannot_compile(x):\n",
    "    return x['key']\n",
    "\n",
    "cannot_compile(dict(key='hey heres your value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c96225",
   "metadata": {
    "id": "f0c96225"
   },
   "source": [
    "<a name=\"CuDF\"></a>\n",
    "## CuDF and CuML\n",
    "\n",
    "CuDF is develeopped by rapidsai (https://rapids.ai/) and like CuPY the goal is to have the features of pandas using GPUs. CuML is also develeopped by rapidsai (https://rapids.ai/) and this time the library we want to apply GPUs is Scikit-learn. Installing them on google collab is a bit complex so we will directly use their google collab :https://colab.research.google.com/drive/1rY7Ln6rEE1pOlfSHCYOVaqt8OvDO35J0#forceEdit=true&sandboxMode=true&scrollTo=JI7UTXbhaBon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0888630d",
   "metadata": {
    "id": "0888630d"
   },
   "source": [
    "<a name=\"TODO\"></a>\n",
    "## TODO\n",
    "\n",
    "code review: \n",
    "- https://www.programcreek.com/python/example/111769/cupy.ElementwiseKernel"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Chapter3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
